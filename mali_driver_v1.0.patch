diff -urpN odxu3_3.10.72_original/arch/arm/boot/dts/exynos5422_evt0.dtsi transcl/arch/arm/boot/dts/exynos5422_evt0.dtsi
--- odxu3_3.10.72_original/arch/arm/boot/dts/exynos5422_evt0.dtsi	2018-02-21 16:00:24.988623687 +0900
+++ transcl/arch/arm/boot/dts/exynos5422_evt0.dtsi	2018-02-21 15:57:48.845654116 +0900
@@ -91,6 +91,16 @@
 		};
 	};
 
+    arm-pmu {
+        /*  compatible = "arm,cortex-a15-pmu";
+            interrupt-parent = <&combiner>;
+            interrupts = <1 2>, <7 0>, <16 6>, <19 2>;
+        */
+        ompatible = "arm,cortex-a7-pmu";
+        interrupt-parent = <&gic>;
+        interrupts = <0 192 4>, <0 193 4>, <0 194 4>, <0 195 4>;
+    };
+
 	watchdog@10020000 {
 		compatible = "samsung,s3c2410-wdt";
 		reg = <0x101D0000 0x100>;
Binary files odxu3_3.10.72_original/arch/arm/boot/zImage-dtb and transcl/arch/arm/boot/zImage-dtb differ
diff -urpN odxu3_3.10.72_original/arch/arm/include/asm/unistd.h transcl/arch/arm/include/asm/unistd.h
--- odxu3_3.10.72_original/arch/arm/include/asm/unistd.h	2018-02-21 16:00:25.012623529 +0900
+++ transcl/arch/arm/include/asm/unistd.h	2018-02-21 15:57:48.868653964 +0900
@@ -15,7 +15,7 @@
 
 #include <uapi/asm/unistd.h>
 
-#define __NR_syscalls  (384)
+#define __NR_syscalls  (380)
 #define __ARM_NR_cmpxchg		(__ARM_NR_BASE+0x00fff0)
 
 #define __ARCH_WANT_STAT64
diff -urpN odxu3_3.10.72_original/arch/arm/include/uapi/asm/unistd.h transcl/arch/arm/include/uapi/asm/unistd.h
--- odxu3_3.10.72_original/arch/arm/include/uapi/asm/unistd.h	2018-02-21 16:00:25.013623522 +0900
+++ transcl/arch/arm/include/uapi/asm/unistd.h	2018-02-21 15:57:48.869653958 +0900
@@ -406,7 +406,6 @@
 #define __NR_process_vm_writev		(__NR_SYSCALL_BASE+377)
 #define __NR_kcmp			(__NR_SYSCALL_BASE+378)
 #define __NR_finit_module		(__NR_SYSCALL_BASE+379)
-#define __NR_mem_alloc_test		(__NR_SYSCALL_BASE+382)
 
 /*
  * This may need to be greater than __NR_last_syscall+1 in order to
diff -urpN odxu3_3.10.72_original/arch/arm/kernel/calls.S transcl/arch/arm/kernel/calls.S
--- odxu3_3.10.72_original/arch/arm/kernel/calls.S	2018-02-21 16:00:25.014623516 +0900
+++ transcl/arch/arm/kernel/calls.S	2018-02-21 15:57:48.869653958 +0900
@@ -389,11 +389,6 @@
 		CALL(sys_process_vm_writev)
 		CALL(sys_kcmp)
 		CALL(sys_finit_module)
-/* 380 */
-		CALL(sys_ni_syscall)
-		CALL(sys_ni_syscall)
-		CALL(sys_mem_alloc_test)
-		CALL(sys_ni_syscall)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
 #define syscalls_counted
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/Kbuild transcl/drivers/gpu/arm/midgard/Kbuild
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/Kbuild	2018-02-21 16:00:25.637619404 +0900
+++ transcl/drivers/gpu/arm/midgard/Kbuild	2018-02-21 15:57:49.495649827 +0900
@@ -109,6 +109,11 @@ SRC := \
 	mali_kbase_sync.c \
 	mali_kbase_sync_user.c \
 	mali_kbase_replay.c \
+    mali_custom_ioctl.c \
+    mali_custom_proc.c \
+    mali_custom_eval.c \
+    mali_custom_sched.c \
+    mali_custom_snap.c \
 
 ifeq ($(MALI_CUSTOMER_RELEASE),0)
 SRC += \
@@ -223,6 +228,6 @@ endif
 obj-$(CONFIG_MALI_MIDGARD) += mali_kbase.o
 
 # Tell the Linux build system to enable building of our .c files
-mali_kbase-y := $(SRC:.c=.o)
+mali_kbase-y := $(SRC:.c=.o) mali_custom_neon.o
 
 
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_eval.c transcl/drivers/gpu/arm/midgard/mali_custom_eval.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_eval.c	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_eval.c	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,187 @@
+#include <mali_kbase.h>
+#include <mali_kbase_mem.h>
+#include <linux/ktime.h>
+
+
+void device_trace_init(kbase_device *kbdev){
+	kbdev->tInfo.nr_ctx_id = 0;
+}
+
+void context_trace_init(kbase_context *kctx){
+	kctx->tInfo.ctx_id = kctx->kbdev->tInfo.nr_ctx_id;
+	kctx->kbdev->tInfo.nr_ctx_id++;
+	
+	kctx->tInfo.nr_atom_id = 0;
+	kctx->tInfo.nr_reg_id = 0;
+
+	kctx->tInfo.task = current;
+	if(!strcmp(current->comm, "micro_bench") || 
+		!strcmp(current->comm, "kernel1") || 
+		!strcmp(current->comm, "kernel2") || 
+		!strcmp(current->comm, "kernel3") || 
+		!strcmp(current->comm, "granularity"))
+		kctx->tInfo.is_micro = 1;
+
+	kctx->tInfo.app_start = ktime_get();
+
+	kctx->tInfo.total_nr_pages = 0;
+	kctx->tInfo.max_nr_pages = 0;
+	kctx->tInfo.nr_spages = 0;
+	kctx->tInfo.nr_preempt = 0;
+}
+
+void context_trace_init_reg(kbase_context *kctx, kbase_va_region *reg){
+	reg->tInfo.reg_id = kctx->tInfo.nr_reg_id;
+	kctx->tInfo.nr_reg_id++;
+	reg->tInfo.start = ktime_get();
+}
+void context_trace_alloc_done_reg(kbase_context *kctx, kbase_va_region *reg){
+	ktime_t alloc_reg_time;
+	reg->tInfo.end = ktime_get();
+	alloc_reg_time = ktime_sub(reg->tInfo.end, reg->tInfo.start);
+	kctx->tInfo.mtimes = ktime_add(kctx->tInfo.mtimes, alloc_reg_time);
+	
+	kctx->tInfo.total_nr_pages += reg->alloc->nents;
+	
+	if(kctx->tInfo.total_nr_pages > kctx->tInfo.max_nr_pages)
+		kctx->tInfo.max_nr_pages = kctx->tInfo.total_nr_pages;
+}
+
+void context_trace_release_reg(kbase_context *kctx, u32 nr_pages){
+	kctx->tInfo.total_nr_pages -= nr_pages;
+}
+
+void context_trace_release(kbase_context *kctx){
+	kctx->tInfo.app_end = ktime_get();
+}
+
+void context_trace_preemption(kbase_context *kctx){
+	kctx->tInfo.nr_preempt++;
+}
+
+void job_trace_init(kbase_jd_atom *katom){
+
+	if(katom->core_req & BASE_JD_REQ_SOFT_JOB)
+		return;
+
+	katom->tInfo.atom_id = katom->kctx->tInfo.nr_atom_id;
+	katom->kctx->tInfo.nr_atom_id++;
+
+	katom->tInfo.is_head = 0;
+	katom->tInfo.nr_spages = 0;
+	
+	katom->tInfo.atom_req_start = ktime_get();
+}
+
+void job_trace_run_start(kbase_jd_atom *katom){
+	u8 pos;
+	kbase_jm_slot *slot = &katom->kctx->kbdev->jm_slots[1];
+	pos = slot->submitted_head & BASE_JM_SUBMIT_SLOTS_MASK;
+
+	if(slot->submitted[pos] == katom){
+		katom->tInfo.is_head = 1;
+	}else{
+		katom->tInfo.is_head = 0;
+	}
+	katom->tInfo.atom_run_start = ktime_get();
+}
+
+void job_trace_run_int_end(kbase_device *kbdev){
+	u8 pos;
+	kbase_jd_atom *katom;
+	kbase_jm_slot *slot;
+	ktime_t run_time, delay_time;
+	
+	if(kbdev->preempt_slot.submitted_nr)
+		    slot = &kbdev->preempt_slot;
+	else
+		    slot = &kbdev->jm_slots[1];
+
+	
+	pos = slot->submitted_head & BASE_JM_SUBMIT_SLOTS_MASK;
+	katom = slot->submitted[pos];
+	
+	katom->tInfo.atom_run_end = ktime_get();
+	
+	if(!katom->tInfo.is_head){
+		katom->tInfo.atom_run_start = kbdev->tInfo.prev_atom_end;
+
+		//실제 종료 시간과 interrupt 핸들러 호출까지의 시간은 고려 되지 않음.
+	}
+
+	run_time = ktime_sub(katom->tInfo.atom_run_end, katom->tInfo.atom_run_start);
+	delay_time = ktime_sub(katom->tInfo.atom_run_start, katom->tInfo.atom_req_start);
+	
+	katom->tInfo.delay_time = delay_time;
+	katom->tInfo.run_time = run_time;
+	
+	katom->kctx->tInfo.kernel_sched_delay = ktime_add(katom->kctx->tInfo.kernel_sched_delay, delay_time);
+	katom->kctx->tInfo.kernel_total_time = ktime_add(katom->kctx->tInfo.kernel_total_time, run_time);
+
+	kbdev->tInfo.prev_atom_end = katom->tInfo.atom_run_end;
+		/*trace_gpu_custom_bench(
+				katom->kctx->tInfo.task->comm,
+				katom->tInfo.atom_id,
+				ktime_to_ns(katom->tInfo.atom_req_start),
+				ktime_to_ns(katom->tInfo.atom_run_start),
+				ktime_to_ns(katom->tInfo.atom_run_end),
+				ktime_to_ns(delay_time),
+				0,//kctx->nr_spages,
+				0,//kctx->nr_mpages,
+				0,
+				katom->core_req,//kctx->nr_preempt,
+			        (u8)katom->tInfo.is_head//kctx->nr_dep_job
+				);*/
+}
+
+void job_trace_run_end(kbase_jd_atom *katom){
+}
+
+void job_trace_snapshot_start(kbase_jd_atom *katom){
+	katom->tInfo.stimes_start = ktime_get();
+}
+void job_trace_snapshot_end(kbase_jd_atom *katom){
+	katom->tInfo.stimes_end = ktime_get();
+	katom->tInfo.stimes = ktime_sub(katom->tInfo.stimes_end, katom->tInfo.stimes_start);
+
+	katom->kctx->tInfo.nr_spages+=katom->tInfo.nr_spages; //for total snapshot memory size
+	katom->kctx->tInfo.stimes = ktime_add(katom->kctx->tInfo.stimes, katom->tInfo.stimes);
+}
+void job_trace_snapshot_pages(kbase_jd_atom *katom, u32 nr_pages){
+	katom->tInfo.nr_spages+=nr_pages;
+}
+
+
+void printout_context_trace(kbase_context *kctx){
+	if(!kctx->tInfo.is_micro){
+		trace_gpu_custom_bench(
+				kctx->tInfo.task->comm,
+				kctx->tInfo.ctx_id,
+				ktime_to_ns(ktime_sub(kctx->tInfo.app_end, kctx->tInfo.app_start)),
+				ktime_to_ns(kctx->tInfo.stimes),
+				ktime_to_ns(kctx->tInfo.kernel_sched_delay),
+				ktime_to_ns(kctx->tInfo.kernel_total_time),
+				kctx->tInfo.nr_spages,
+				kctx->tInfo.max_nr_pages,//kctx->nr_mpages,
+				kctx->tInfo.nr_atom_id,//kctx->nr_kernel,
+				kctx->tInfo.nr_preempt,
+				kctx->tInfo.nr_preempted
+				);
+	}
+}
+
+void printout_job_trace(kbase_jd_atom *katom){
+		trace_gpu_custom_bench(
+				katom->kctx->tInfo.task->comm,
+				katom->tInfo.atom_id,
+				ktime_to_ns(katom->tInfo.atom_req_start),
+				ktime_to_ns(katom->tInfo.stimes_start),
+				ktime_to_ns(katom->tInfo.stimes_end),
+				ktime_to_ns(katom->tInfo.run_time),//time
+				0,//kctx->nr_spages,
+				0,//kctx->nr_mpages,
+				0,
+				katom->core_req,//kctx->nr_preempt,
+			        (u8)katom->tInfo.is_head//kctx->nr_dep_job
+				);
+}
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_eval.h transcl/drivers/gpu/arm/midgard/mali_custom_eval.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_eval.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_eval.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,76 @@
+#include <linux/ktime.h>
+#include <linux/sched.h>
+
+typedef struct kbase_va_region kbase_va_region;
+
+typedef struct device_trace{
+    u32 nr_ctx_id;
+    ktime_t prev_atom_end;
+}device_trace;
+
+typedef struct context_trace{
+    u32 ctx_id;
+    u64 nr_atom_id;
+    u32 nr_reg_id;
+
+    struct task_struct *task;
+    u8 is_micro;
+    
+    ktime_t app_start, app_end;
+    ktime_t kernel_total_time;
+    ktime_t kernel_sched_delay;
+    ktime_t stimes;
+    ktime_t mtimes;
+   
+    u32 total_nr_pages;
+    u32 max_nr_pages;
+
+    u32 nr_spages; 
+    u32 nr_preempt;
+    u32 nr_preempted;
+
+}context_trace;
+
+typedef struct job_trace{
+    u64 atom_id;
+
+    u8 is_head;
+    
+    ktime_t atom_req_start, atom_req_end;
+    ktime_t atom_run_start, atom_run_end;
+    ktime_t delay_time, run_time;
+    ktime_t stimes, stimes_start, stimes_end;
+    
+    u32 nr_spages;
+
+}job_trace;
+
+typedef struct reg_trace{
+    u32 reg_id;
+    ktime_t start, end;
+}reg_trace;
+
+
+void device_trace_init(kbase_device *kbdev);
+void context_trace_init(kbase_context *kctx);
+void job_trace_init(kbase_jd_atom *katom);
+
+void context_trace_release(kbase_context *kctx);
+
+void job_trace_run_start(kbase_jd_atom *katom);
+void job_trace_run_int_end(kbase_device *kbdev);
+void job_trace_run_end(kbase_jd_atom *katom);
+void job_trace_snapshot_start(kbase_jd_atom *katom);
+void job_trace_snapshot_end(kbase_jd_atom *katom);
+void job_trace_snapshot_pages(kbase_jd_atom *katom, u32 nr_pages);
+
+
+void context_trace_init_reg(kbase_context *kctx, kbase_va_region *reg);
+void context_trace_alloc_done_reg(kbase_context *kctx, kbase_va_region *reg);
+void context_trace_release_reg(kbase_context *kctx, u32 nr_pages);
+
+void context_trace_preemption(kbase_context *kctx);
+
+
+void printout_context_trace(kbase_context *kctx);
+void printout_job_trace(kbase_jd_atom *katom);
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_ioctl.c transcl/drivers/gpu/arm/midgard/mali_custom_ioctl.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_ioctl.c	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_ioctl.c	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,222 @@
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_mem.h>
+
+#ifdef _TSK_CUSTOM_IOCTL_
+
+void kbase_custom_ctx(struct kbase_context *kctx, u32 ctx_id);
+struct kbase_context* kbase_custom_ctx_find(struct kbase_context *kctx, u32 ctx_id);
+void kbase_debug_dump_registers(kbase_device *kbdev);
+
+void kbase_custom_ioctl(struct kbase_context *kctx, unsigned int cmd, unsigned long arg){
+
+	u32 nr_ioc = _IOC_NR(cmd);
+	u32 size = _IOC_SIZE(cmd);
+	gpu_umsg umsg;
+	struct kbase_device *kbdev = kctx->kbdev;
+
+	switch(nr_ioc){
+		case 0:
+			printk(KERN_ALERT"<gpu> ioctl00\n");
+			break;
+		case 1:
+			printk(KERN_ALERT"<gpu> ioctl01\n");
+
+			break;
+		case 2:
+			printk(KERN_ALERT"<gpu> ioctl02\n");
+			break;
+		case 3:
+			printk(KERN_ALERT"<gpu> ioctl03\n");
+			break;
+		case 4:
+			printk(KERN_ALERT"<gpu> ioctl04\n");
+			kbase_js_try_run_jobs_on_slot(kbdev, 1); 
+			break;
+		case 5:
+			printk(KERN_ALERT"<gpu> ioctl05\n");
+			if(copy_from_user((void*)&umsg, (const void*)arg, size)==0){
+				struct kbase_context *selctx = NULL;
+				struct kbase_jd_atom *selatom = NULL;
+				int is_atom = 0;
+				int js, pos;
+
+				printk(KERN_ALERT"<gpu>Try to hardstop : ctx <%u> atom <%llu>\n", umsg.ctx_id, umsg.atom_id);
+				selctx = kbase_custom_ctx_find(kctx, umsg.ctx_id);
+				if(selctx == NULL){
+
+					printk(KERN_ALERT"<gpu> There is no ctx\n");
+					break;
+				}
+
+				for (js = 0; js < kctx->kbdev->gpu_props.num_job_slots; ++js){
+					for(pos =0; pos < 16; pos++){
+						if(!(kctx->kbdev->jm_slots[js].submitted[pos]==NULL)){
+							 selatom = kctx->kbdev->jm_slots[js].submitted[pos];
+							 if(selatom->tInfo.atom_id == umsg.atom_id){
+								is_atom = 1;
+								//selatom->is_not_preempt = 0;
+								printk(KERN_ALERT"hardstop success : ctx[%u] atom[%llu]\n",selctx->tInfo.ctx_id, selatom->tInfo.atom_id);
+								kbase_job_slot_hardstop(selctx, js, selatom);
+								break;
+							 }
+						}
+					}
+				}
+			}
+			break;
+		case 6:
+			printk(KERN_ALERT"<gpu> ioctl06\n");
+			if(copy_from_user((void*)&umsg, (const void*)arg, size)==0){
+				kbdev->snap_granularity = (u32)umsg.data1;
+				printk(KERN_ALERT"SET snap_granularity : %u (pages)\n", kbdev->snap_granularity);
+
+			}
+			break;
+		case 7:
+			printk(KERN_ALERT"<gpu> ioctl07\n");
+			kbase_debug_dump_registers(kbdev);
+			break;
+		case 8:
+			printk(KERN_ALERT"<gpu> ioctl08\n");
+			if(copy_from_user((void*)&umsg, (const void*)arg, size)==0){
+				kbase_custom_ctx(kctx,umsg.ctx_id);
+			}
+			break;
+		case 9:
+			//for preempt trace(1:eviction latency, 2:preempt & launch delay)
+			printk(KERN_ALERT"<gpu> ioctl09\n");
+#ifdef _TSK_TRACE_EVICTION_
+			if(copy_from_user((void*)&umsg, (const void*)arg, size)==0){
+				kbdev->sw_trace  = (u8)umsg.data1;
+			}
+			printk(KERN_ALERT"switching sched trace : %u\n", kbdev->sw_trace);
+#endif
+			
+			break;
+		default:
+			break;
+	}
+}
+
+struct kbase_context* kbase_custom_ctx_find(struct kbase_context *kctx, u32 ctx_id){
+	int is_ctx = 0;
+	struct kbase_context *selctx;
+	struct list_head *ctx_pool = &kctx->kbdev->js_data.policy.cfs.ctx_queue_head;               
+	struct list_head *scheduled_ctx_pool = &kctx->kbdev->js_data.policy.cfs.scheduled_ctxs_head;
+
+	list_for_each_entry(selctx, ctx_pool, jctx.sched_info.runpool.policy_ctx.cfs.list){
+		if(selctx->tInfo.ctx_id == ctx_id){
+			is_ctx = 1;
+			break;
+		}
+	}
+
+	if(!is_ctx){
+		list_for_each_entry(selctx, scheduled_ctx_pool, jctx.sched_info.runpool.policy_ctx.cfs.list){
+			if(selctx->tInfo.ctx_id == ctx_id){
+				is_ctx = 1;
+				break;
+			}
+		}
+	}
+
+	if(!is_ctx){
+		return NULL;
+	}
+
+	return selctx;
+}
+
+void kbase_custom_ctx(struct kbase_context *kctx, u32 ctx_id){
+
+	struct kbase_device *kbdev;
+	kbase_jd_atom *a;
+	kbase_context *k;
+	u32 cid=~(u32)0x0;
+	kbase_context *selk=NULL;
+
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbasep_js_policy *js_policy;
+	kbasep_js_policy_cfs *policy_info;
+	kbase_jm_slot *jm_slots;
+	                                   
+	struct list_head *job_list;
+	struct list_head *queue_head;
+	int i, j;
+
+	kbdev = kctx->kbdev;
+	cid = ctx_id;
+
+	printk(KERN_ALERT"<gpu> ++++++++GPU Queue INFO+++++++\n");
+
+	printk(KERN_ALERT"<gpu> [KBDEV - slot]\n");
+
+	jm_slots = kctx->kbdev->jm_slots;
+
+	for(i=0;i<3;i++){
+		for(j=0;j<16;j++){
+			if(!(jm_slots[i].submitted[j]==NULL)){
+				a = jm_slots[i].submitted[j];
+				printk(KERN_ALERT"<gpu> slot <%d> - submitted <%d> - ctx[%u] atom <%llu> - jc <0x%016llx>\n",i, j, a->kctx->tInfo.ctx_id, a->tInfo.atom_id, a->jc);
+			}
+		}                                                                                                    
+	}
+
+	printk(KERN_ALERT"<gpu> [KBDEV - ctx runpool]\n");
+
+	js_policy = &kctx->kbdev->js_data.policy;
+	policy_info = &js_policy->cfs;
+	queue_head = &policy_info->ctx_queue_head;
+
+	list_for_each_entry(k,queue_head, jctx.sched_info.runpool.policy_ctx.cfs.list){
+		if(k->tInfo.ctx_id == cid)
+			selk = k;
+		printk(KERN_ALERT"<gpu> there is the <%u> ctx\n",k->tInfo.ctx_id);
+	}
+
+	printk(KERN_ALERT"<gpu> [KBDEV - ctx scheduled runpool]\n");
+
+	queue_head = &policy_info->scheduled_ctxs_head;
+
+	list_for_each_entry(k,queue_head, jctx.sched_info.runpool.policy_ctx.cfs.list){
+		if(k->tInfo.ctx_id == cid)
+			selk = k;
+		printk(KERN_ALERT"<gpu> there is the <%u> ctx[%s] (%u)\n",k->tInfo.ctx_id, k->tInfo.task->comm, (u32)atomic_read(&k->process_preempt));
+	}
+
+	printk(KERN_ALERT"<gpu> <ioctl> [KCTX - atom pool per slot]\n");
+
+	if(selk==NULL)
+		selk = kctx;
+
+	ctx_info = &selk->jctx.sched_info.runpool.policy_ctx.cfs;
+	for(i=0;i<7;i++){
+		job_list = &ctx_info->job_list_head[i];
+		
+		if(list_empty(job_list))
+			printk(KERN_ALERT"<gpu> <ioctl> job_list <%d> empty\n", i);
+		else{
+			list_for_each_entry(a,job_list, sched_info.cfs.list){
+				printk(KERN_ALERT"<gpu> <ioctl> there is the <%llu> atom - jc <0x%016llx>\n", a->tInfo.atom_id, a->jc);
+			}
+		}
+	}
+	
+	printk(KERN_ALERT"<gpu> <ioctl> [Register Dump]\n");
+	printk(KERN_ALERT"<gpu> JOB_SLOT STATUS : 0x%08x | JOB_IRQ_RAWSTAT=0x%08x\n"
+				, kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_STATUS),NULL)
+				, kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT), NULL));
+	printk(KERN_ALERT"<gpu> JSn_HEAD : 0x%llx\n", ((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_HEAD_LO), NULL))
+			        | (((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_HEAD_HI), NULL)) << 32));
+	printk(KERN_ALERT"<gpu> JSn_HEAD_NEXT : 0x%llx\n", ((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_HEAD_NEXT_LO), NULL))
+			        | (((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_HEAD_NEXT_HI), NULL)) << 32));
+	printk(KERN_ALERT"<gpu> JSn_COMMAND : 0x%08x | JSn_COMMAND_NEXT : 0x%08x\n"
+			        , kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_COMMAND), NULL)
+				, kbase_reg_read(kbdev, JOB_SLOT_REG(1, JSn_COMMAND_NEXT), NULL));
+
+
+
+	printk(KERN_ALERT"<gpu> +++++++++++++++++++++++++++++\n"); 
+}
+#endif
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_ioctl.h transcl/drivers/gpu/arm/midgard/mali_custom_ioctl.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_ioctl.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_ioctl.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,24 @@
+#define IOCTL_GPU_MAGIC (100)
+
+#define IOCTL_0 IOWR (IOCTL_GPU_MAGIC, 0, gpu_umsg )
+#define IOCTL_1 IOWR (IOCTL_GPU_MAGIC, 1, gpu_umsg )
+#define IOCTL_2 IOWR (IOCTL_GPU_MAGIC, 2, gpu_umsg )
+#define IOCTL_3 IOWR (IOCTL_GPU_MAGIC, 3, gpu_umsg )
+#define IOCTL_4 IOWR (IOCTL_GPU_MAGIC, 4, gpu_umsg )
+#define IOCTL_5 IOWR (IOCTL_GPU_MAGIC, 5, gpu_umsg )
+#define IOCTL_6 IOWR (IOCTL_GPU_MAGIC, 6, gpu_umsg )
+#define IOCTL_7 IOWR (IOCTL_GPU_MAGIC, 7, gpu_umsg )
+#define IOCTL_8 IOWR (IOCTL_GPU_MAGIC, 8, gpu_umsg )
+#define IOCTL_9 IOWR (IOCTL_GPU_MAGIC, 9, gpu_umsg )
+
+#define IOCTL_GPU_MAXNR 10
+
+typedef struct{
+    u32 ctx_id;
+    u64 atom_id;
+    u64 data1;
+    u64 data2;
+}__attribute__((packed)) gpu_umsg;
+
+
+void kbase_custom_ioctl(struct kbase_context *kctx, unsigned int cmd, unsigned long arg);
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_neon.S transcl/drivers/gpu/arm/midgard/mali_custom_neon.S
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_neon.S	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_neon.S	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,147 @@
+/*
+ * NEON code contributed by Siarhei Siamashka <siarhei.siamashka@nokia.com>.
+ * Origin: http://sourceware.org/ml/libc-ports/2009-07/msg00003.html
+ *
+ * The GNU C Library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License.
+ *
+ * tweaked for Android by Jim Huang <jserv@0xlab.org>
+ */
+
+//#include <cpu-features.h>
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+
+		.text
+		.fpu	neon
+
+		//.global memcpy_neon
+		//.type memcpy_neon, %function
+		//.align 4
+
+/*
+ * ENABLE_UNALIGNED_MEM_ACCESSES macro can be defined to permit the use
+ * of unaligned load/store memory accesses supported since ARMv6. This
+ * will further improve performance, but can purely theoretically cause
+ * problems if somebody decides to set SCTLR.A bit in the OS kernel
+ * (to trap each unaligned memory access) or somehow mess with strongly
+ * ordered/device memory.
+ */
+#define ENABLE_UNALIGNED_MEM_ACCESSES 1
+
+#define NEON_MAX_PREFETCH_DISTANCE 320
+
+ENTRY(memcpy_neon)
+
+//memcpy_neon:
+	//.fnstart
+		mov	ip, r0
+		cmp	r2, #16
+		blt     4f	@ Have less than 16 bytes to copy
+
+		@ First ensure 16 byte alignment for the destination buffer
+		tst	r0, #0xF
+		beq	2f
+		tst	r0, #1
+		ldrneb	r3, [r1], #1
+		strneb	r3, [ip], #1
+		subne	r2, r2, #1
+		tst	ip, #2
+#ifdef ENABLE_UNALIGNED_MEM_ACCESSES
+		ldrneh	r3, [r1], #2
+		strneh	r3, [ip], #2
+#else
+		ldrneb	r3, [r1], #1
+		strneb	r3, [ip], #1
+		ldrneb	r3, [r1], #1
+		strneb	r3, [ip], #1
+#endif
+		subne	r2, r2, #2
+
+		tst	ip, #4
+		beq	1f
+		vld4.8	{d0[0], d1[0], d2[0], d3[0]}, [r1]!
+		vst4.8	{d0[0], d1[0], d2[0], d3[0]}, [ip, :32]!
+		sub	r2, r2, #4
+1:
+		tst	ip, #8
+		beq	2f
+		vld1.8	{d0}, [r1]!
+		vst1.8	{d0}, [ip, :64]!
+		sub	r2, r2, #8
+2:
+		subs	r2, r2, #32
+		blt	3f
+		mov	r3, #32
+
+		@ Main copy loop, 32 bytes are processed per iteration.
+		@ ARM instructions are used for doing fine-grained prefetch,
+		@ increasing prefetch distance progressively up to
+		@ NEON_MAX_PREFETCH_DISTANCE at runtime
+1:
+		vld1.8	{d0-d3}, [r1]!
+		cmp	r3, #(NEON_MAX_PREFETCH_DISTANCE - 32)
+		pld	[r1, r3]
+		addle	r3, r3, #32
+		vst1.8	{d0-d3}, [ip, :128]!
+		sub	r2, r2, #32
+		cmp	r2, r3
+		bge	1b
+		cmp	r2, #0
+		blt	3f
+1:		@ Copy the remaining part of the buffer (already prefetched)
+		vld1.8	{d0-d3}, [r1]!
+		subs	r2, r2, #32
+		vst1.8	{d0-d3}, [ip, :128]!
+		bge	1b
+3:		@ Copy up to 31 remaining bytes
+		tst	r2, #16
+		beq	4f
+		vld1.8	{d0, d1}, [r1]!
+		vst1.8	{d0, d1}, [ip, :128]!
+4:
+		@ Use ARM instructions exclusively for the final trailing part
+		@ not fully fitting into full 16 byte aligned block in order
+		@ to avoid "ARM store after NEON store" hazard. Also NEON
+		@ pipeline will be (mostly) flushed by the time when the
+		@ control returns to the caller, making the use of NEON mostly
+		@ transparent (and avoiding hazards in the caller code)
+
+#ifdef ENABLE_UNALIGNED_MEM_ACCESSES
+		movs	r3, r2, lsl #29
+		ldrcs	r3, [r1], #4
+		strcs	r3, [ip], #4
+		ldrcs	r3, [r1], #4
+		strcs	r3, [ip], #4
+		ldrmi	r3, [r1], #4
+		strmi	r3, [ip], #4
+		movs	r2, r2, lsl #31
+		ldrcsh	r3, [r1], #2
+		strcsh	r3, [ip], #2
+		ldrmib	r3, [r1], #1
+		strmib	r3, [ip], #1
+#else
+		movs	r3, r2, lsl #29
+		bcc	1f
+	.rept	8
+		ldrcsb	r3, [r1], #1
+		strcsb	r3, [ip], #1
+	.endr
+1:
+		bpl	1f
+	.rept	4
+		ldrmib	r3, [r1], #1
+		strmib	r3, [ip], #1
+	.endr
+1:
+		movs	r2, r2, lsl #31
+		ldrcsb	r3, [r1], #1
+		strcsb	r3, [ip], #1
+		ldrcsb	r3, [r1], #1
+		strcsb	r3, [ip], #1
+		ldrmib	r3, [r1], #1
+		strmib	r3, [ip], #1
+#endif
+		bx	lr
+	//.fnend
+ENDPROC(memcpy_neon)
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_proc.c transcl/drivers/gpu/arm/midgard/mali_custom_proc.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_proc.c	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_proc.c	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,172 @@
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_mem.h>
+
+#include <linux/highmem.h>
+#include <linux/mempool.h>
+#include <linux/mm.h>
+#include <linux/atomic.h> 
+  
+#include <linux/rbtree.h>
+#include <linux/fs.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+
+#ifdef _TSK_CUSTOM_PROC_
+
+struct kbase_device *proc_kbdev;
+struct kbase_context *proc_kctx;
+
+struct kbase_context * proc_ctx_search(long proc_ctx_id){
+
+	int is_find=0;
+	struct kbase_context *sctx=NULL;
+
+	list_for_each_entry(sctx,&proc_kbdev->proc_ctx_list, proc_ctx_list_node){
+		if(sctx->tInfo.ctx_id == (u32)proc_ctx_id){
+			is_find = 1;                 
+			break;                       
+		}
+	}
+
+	if(!is_find){
+		printk(KERN_ALERT"<gpu> proc_ctx_id(%ld) can't be found!\n", proc_ctx_id);
+		return NULL;
+	}
+	return sctx;
+	
+}
+
+static void *s_start(struct seq_file *m, loff_t *pos){
+
+	int fcount=0;
+	struct kbase_va_region *reg;
+	struct kbase_cpu_mapping *map;
+	struct rb_node *node = NULL;
+	struct rb_root *rbroot = NULL;
+	size_t s;
+
+	if(proc_kctx==NULL){
+		printk(KERN_ALERT"<gpu> proc_ctx_id can't be found!\n");
+		return NULL;
+	}
+	if(*pos == 0){ 
+		seq_printf(m,"|   CTX ID    : %05u            |\n", proc_kctx->tInfo.ctx_id);
+	}
+
+	rbroot = &proc_kctx->reg_rbtree;
+
+	for(node = rb_first(rbroot) ; node ; node = rb_next(node)){  
+		reg = rb_entry(node, struct kbase_va_region, rblink);
+		s = kbase_reg_current_backed_size(reg);              
+		if(s){
+			if(fcount == *pos){
+				seq_printf(m,"==================================\n");
+				seq_printf(m,"|   region    :    %05d - %05zu |\n", reg->tInfo.reg_id, reg->alloc->nents);
+				seq_printf(m,"|   PFN       : %16llx |\n", reg->start_pfn);
+				map = list_entry(&reg->alloc->mappings, kbase_cpu_mapping, mappings_list);
+				seq_printf(m,"|   MAP       : %16lx |\n", map->vm_start);
+				if ((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_SAME_VA){
+					seq_printf(m,"|   type      :     ZONE_SAME_VA |\n");
+				}else if ((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_EXEC){
+					seq_printf(m,"|   type      :     ZONE_EXEC_VA |\n");
+				}else if((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_CUSTOM_VA){
+					seq_printf(m,"|   type      :     ZONE_CUST_VA |\n");
+				}
+
+				return reg;
+			}
+			fcount++;
+		}
+	}
+
+	return NULL;
+}
+
+static void *s_next(struct seq_file *m, void *p, loff_t *pos){
+	return NULL;
+}
+
+static void s_stop(struct seq_file *m, void *p){
+}                                               
+
+
+static int s_show(struct seq_file *m, void *p){
+	struct kbase_va_region *reg;
+	void *mapping;
+	u32 *mapping_32;
+	int i,j;
+
+	//tmp
+	u32 nr_limited_pages;
+
+	reg = (struct kbase_va_region*)p;
+
+	if(reg->alloc->nents < 10){
+		nr_limited_pages = reg->alloc->nents; 
+	}else{
+		nr_limited_pages = 10;
+	}
+	for(i=0;i<nr_limited_pages;i++){//reg->alloc->nents
+		mapping = kmap_atomic(pfn_to_page(PFN_DOWN(reg->alloc->pages[i])));
+		mapping_32 = mapping;
+		seq_printf(m, "==================================\n");
+		seq_printf(m, "| page number : %05d            |\n", i);
+		seq_printf(m, "==================================\n");
+		for(j=0;j<1024;j++){
+			
+			seq_printf(m, "|%04x| 0x%08x ", j*4 ,*(mapping_32+j));
+			
+			if(((j+1)%10)==0)            
+				seq_printf(m, "|\n");
+		}
+		seq_printf(m, "\n\n");
+		__kunmap_atomic(mapping);
+	}
+	return 0;
+}
+
+
+static const struct seq_operations mali_mem_seq_op = {
+	.start = s_start, 
+	.next = s_next,
+	.stop = s_stop,
+	.show = s_show,
+};                                                    
+
+
+static int mali_mem_seq_open(struct inode *inode, struct file *filp)
+{
+	return seq_open(filp, &mali_mem_seq_op);                    
+}
+
+ssize_t mali_mem_write(struct file *f, const char __user *buffer, size_t count, loff_t * data){
+
+	//struct kbase_device *kbdev = (struct kbase_device *)data;
+	proc_kctx = proc_ctx_search(simple_strtol(buffer,NULL, 10));
+	if(proc_kctx!=NULL)
+		printk(KERN_ALERT"<gpu> select proc_ctx_id : %u\n", proc_kctx->tInfo.ctx_id);
+	return count;
+}
+
+static const struct file_operations mali_mem_proc_op ={
+	.open = mali_mem_seq_open,
+	.read = seq_read,
+	.write = mali_mem_write,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+int kbase_mem_proc_init(struct kbase_device *kbdev){
+	if(!proc_create_data("mali_mem_dump",S_IRWXU,NULL, &mali_mem_proc_op, kbdev))
+		return 1;
+	proc_kbdev = kbdev;
+	proc_kctx = NULL;
+	INIT_LIST_HEAD(&kbdev->proc_ctx_list);
+	return 0; 
+}
+void kbase_mem_proc_exit(void){ 
+	remove_proc_entry("mali_mem_dump", NULL);
+}
+#endif
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_proc.h transcl/drivers/gpu/arm/midgard/mali_custom_proc.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_proc.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_proc.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,2 @@
+int kbase_mem_proc_init(struct kbase_device *kbdev);
+void kbase_mem_proc_exit(void);
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_sched.c transcl/drivers/gpu/arm/midgard/mali_custom_sched.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_sched.c	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_sched.c	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,356 @@
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_mem.h>
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <mali_kbase_jm.h>
+
+#ifdef _TSK_CUSTOM_SCHED_
+
+void sc_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js){
+	unsigned long flags;
+	kbase_context *kctx;
+	u32 cfg;
+	u64 jc_head = katom->jc;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(katom);
+       
+	kctx = katom->kctx;
+	spin_lock_irqsave(&katom->run_lock, flags);
+
+	if(katom->run_lock_flags){
+		//trace_gpu_custom("submit atom - stop", ktime_to_ns(ktime_get()), kctx->ctx_id, katom->atom_id, (u32)0, (u32)katom->run_lock_flags);
+		katom->run_lock_flags--;
+		spin_unlock_irqrestore(&katom->run_lock, flags);
+		return;
+	}else{
+		//trace_gpu_custom("submit atom - go", ktime_to_ns(ktime_get()), kctx->ctx_id, katom->atom_id, (u32)0, (u32)katom->run_lock_flags);
+		katom->run_lock_flags++;
+	}
+
+	/* Command register must be available */                                               
+	KBASE_DEBUG_ASSERT(kbasep_jm_is_js_free(kbdev, js, kctx));                             
+	/* Affinity is not violating */                                                        
+	//kbase_js_debug_log_current_affinities(kbdev);
+	KBASE_DEBUG_ASSERT(!kbase_js_affinity_would_violate(kbdev, js, katom->affinity));      
+	                                                                                       
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), jc_head & 0xFFFFFFFF, kctx);
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), jc_head >> 32, kctx);       
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_LO), katom->affinity & 0xFFFFFFFF, kctx);
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_HI), katom->affinity >> 32, kctx);       
+
+	/* start MMU, medium priority, cache clean/flush on end, clean/flush on start */                                                                      
+	cfg = kctx->as_nr | JSn_CONFIG_END_FLUSH_CLEAN_INVALIDATE | JSn_CONFIG_START_MMU | JSn_CONFIG_START_FLUSH_CLEAN_INVALIDATE | JSn_CONFIG_THREAD_PRI(8);
+
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION)) {
+		if (!kbdev->jm_slots[js].job_chain_flag) {
+			cfg |= JSn_CONFIG_JOB_CHAIN_FLAG;
+			katom->atom_flags |= KBASE_KATOM_FLAGS_JOBCHAIN;
+			kbdev->jm_slots[js].job_chain_flag = MALI_TRUE;
+		} else {
+			katom->atom_flags &= ~KBASE_KATOM_FLAGS_JOBCHAIN;
+			kbdev->jm_slots[js].job_chain_flag = MALI_FALSE;
+		}
+	}
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_CONFIG_NEXT), cfg, kctx);
+
+	katom->start_timestamp = ktime_get();                                                                                                                 
+	                                                                                                                                                      
+	/* GO ! */
+	KBASE_LOG(2, kbdev->dev, 
+			"JS: Submitting atom %p from ctx %p to js[%d] with head=0x%llx, affinity=0x%llx", katom, kctx, js, jc_head, katom->affinity);
+	                                                                                                                                                      
+	KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_SUBMIT, kctx, katom, jc_head, js, (u32) katom->affinity); 
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_START, katom->kctx);
+	
+	spin_unlock_irqrestore(&katom->run_lock, flags);
+#ifdef _TSK_CUSTOM_TRACE_
+	job_trace_run_start(katom);
+#endif
+#ifdef _TSK_TRACE_EVICTION_
+	if(kbdev->sw_trace == 3 && katom->eviction_stat == 3){//run preemption - launch
+		trace_gpu_custom_bench("PRUN-LAUNCH",katom->eviction_stat, ktime_to_ns(ktime_sub(ktime_get(), katom->launch_time)), 
+			ktime_to_ns(katom->p_delay), 0, 0, 0, 0, kbdev->run_evict, (u32)kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS),NULL), (u32)katom->tInfo.atom_id);
+	}else if(kbdev->sw_trace == 4 && katom->eviction_stat == 2){//snap cancel-launch
+		trace_gpu_custom_bench("PSNAP-LAUNCH",katom->eviction_stat, ktime_to_ns(ktime_sub(ktime_get(), katom->launch_time)), 
+			ktime_to_ns(katom->p_delay), 0, 0, 0, 0, kbdev->run_evict, (u32)kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS),NULL), (u32)katom->tInfo.atom_id);
+	}
+#endif
+
+	//trace_gpu_custom("submit atom", ktime_to_ns(ktime_get()), kctx->ctx_id, katom->atom_id, (u32)spin_is_locked(&katom->run_lock), (u32)0);
+
+}
+
+//lock 문제 발생 가능성 있지만, bit 단위로 보면 상관 없는 서로 다른 비트 접근한다.
+u32 sc_active_jobs(kbase_jm_slot *slot){
+	int i;
+	u32 nr_jobs = 0;
+	struct kbase_jm_slot *preempt_slot = &slot->kbdev->preempt_slot;
+
+	nr_jobs += slot->submitted_nr;
+	nr_jobs += preempt_slot->submitted_nr;
+
+	//trace_gpu_custom("done - irq", ktime_to_ns(ktime_get()), 0, 0, (u32)preempt_slot->submitted_nr, (u32)slot->submitted_nr);
+	for(i=0;i<slot->submitted_nr;i++){//active slot
+		if(!(slot->submitted[(slot->submitted_head + i) &15]->sched_stat & (SCHED_STAT_DONE | SCHED_STAT_PRUN))){
+	//		trace_gpu_custom("done - irq - 1", ktime_to_ns(ktime_get()), 0, 0, (u32)i, (u32)slot->submitted[(slot->submitted_head + i) &15]->sched_stat);
+			nr_jobs--;
+		}
+
+	}
+	
+	return nr_jobs;
+}
+
+//lock 문제 없음.
+u8 sc_is_preempt(kbase_jm_slot *slot){
+
+	kbase_jd_atom *katom;
+
+	if(slot->kbdev->preempt_slot.submitted_nr)
+		slot = &slot->kbdev->preempt_slot;
+
+	katom = slot->submitted[slot->submitted_head & BASE_JM_SUBMIT_SLOTS_MASK];
+	
+	if(katom == NULL){
+		//trace_gpu_custom("done error : atom null", ktime_to_ns(ktime_get()), 0, 0, (u32)slot->submitted_nr, (u32)0);
+		return 1;
+	}
+
+	katom->sched_stat &= ~SCHED_STAT_RUN;
+
+	if(katom->sched_stat & SCHED_STAT_PMASK){
+		return 1;
+	}else{
+		katom->sched_stat |= SCHED_STAT_DONE;
+		return 0;
+	}
+}
+
+//head slot의 경우는 100프로 동시 접근 가능하다.
+int sc_preempt(struct kbase_jd_atom *katom){
+
+	unsigned long sched_flags, run_flags;
+	struct kbase_device *kbdev = katom->kctx->kbdev;
+	kbase_jm_slot *slot;
+	u8 jobs_submitted;
+	kbase_jm_slot *preempt_slot = &kbdev->preempt_slot;
+	struct snapshot_kthread_context *sctx = &kbdev->snapshot_ctx.kthread_ctx[kbdev->snapshot_ctx.skthread_head];
+
+	struct kbase_jd_atom *slot_atom, *dequeued_katom;
+	int atom_priority;
+	u32 preempt_mode = 0;
+	int i, js=1;
+
+#ifdef _TSK_TRACE_EVICTION_
+	ktime_t p_delay = ktime_get();
+#endif
+
+	atom_priority = katom->kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority;
+
+	slot = &katom->kctx->kbdev->jm_slots[js];
+	jobs_submitted = slot->submitted_nr;
+
+	//trace_gpu_custom("sched - start", ktime_to_ns(ktime_get()), katom->kctx->ctx_id, katom->atom_id, (u32)jobs_submitted, (u32) LIMIT_SUBMITTED_NR);
+	for(i=LIMIT_SUBMITTED_NR-1;i>=0;i--){
+
+		slot_atom = slot->submitted[(slot->submitted_head + i) & 15];
+
+		if(slot_atom != NULL){
+
+			if(atom_priority < slot_atom->kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority){
+
+#ifdef _TSK_CUSTOM_TRACE_
+				context_trace_preemption(katom->kctx);
+#endif
+				preempt_mode |=  (1<<i);
+				atomic_inc(&slot_atom->kctx->process_preempt);
+
+				spin_lock_irqsave(&sctx->sched_lock, sched_flags);
+
+				switch(slot_atom->sched_stat & SCHED_STAT_SMASK){
+				case SCHED_STAT_SLOT_READY:
+					/*Job이 Slot에 대기하고 있는 경우(head가 아닌경우)*/
+					
+					slot_atom->sched_stat |= SCHED_STAT_PSLOT;
+					slot_atom->kctx->tInfo.nr_preempted++;
+					dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+					slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;
+
+					if(dequeued_katom != slot_atom){
+						printk(KERN_ALERT"slot atom incorrect");
+						//발생 안하겠지만 실수 방지용.
+					}
+
+					kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+#ifdef _TSK_TRACE_EVICTION_
+					katom->eviction_stat = 1;
+					katom->patom[i] = dequeued_katom->atom_id;
+#endif
+
+					break;
+				case SCHED_STAT_SNAP:
+
+					atomic_inc(&slot_atom->kctx->process_preempt);
+					/* snapshot thread와 enqueue 모두에서 -- 해줘야 정상 스케줄링 가능.
+					 * 딱 이 경우만 동시 접근 가능하기 때문에 atomic 설정
+					 * psnap, psced로 나눈 이유는 enqueue와 kthread 종료 수행을 독립적으로 하기 위해서.
+					 * psnap은 enqueue시에 초기화, psced는 kthread에서 초기화.
+					 * process_preempt는 양쪽에서 --해줘야 정상 preempt 된것으로 보고 스케줄링.
+					 */
+
+#ifdef _TSK_TRACE_EVICTION_
+					slot_atom->snapc_stat = 1;
+					slot_atom->snapc_time = ktime_get();
+#endif
+					set_bit(SCHED_STAT_PSCED_BIT, &slot_atom->sched_stat);
+
+					//kthread와 동시 접근 가능한 비트이므로
+					slot_atom->sched_stat |= SCHED_STAT_PSNAP;
+					slot_atom->kctx->tInfo.nr_preempted++;
+
+					katom->sched_stat |= SCHED_STAT_PCER;
+					
+					dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+					slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;
+
+					if(dequeued_katom != slot_atom){
+						printk(KERN_ALERT"slot atom incorrect");
+					}
+
+					kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+#ifdef _TSK_TRACE_EVICTION_
+					katom->eviction_stat = 2;
+					katom->patom[i] = dequeued_katom->atom_id;
+#endif
+					
+					break;
+				case SCHED_STAT_RUN:
+					/*Job이 GPU에서 동작 중일 경우*/
+					/*이 부분의 논리에 대해서 해석해보면
+					 * 1. 전제 조건으로 무조건 슬롯 head에는 수행 대기라고 생각한다.
+					 * 2. Command가 0이라면 이미 수행중이라고 생각한다.
+					 * 3. Command가 1이라 할지라도 hwsubmit 호출된 놈이라고 생각하기 떄문에 수행을 하고 있는지 안하고 있는지를 판단한 것이다.
+					 * 4. 먼저 커멘드를 없애고 넥스트 레지에 jc가 남아있다면 아직 수행은 하지 않은 것이다. 그러므로 슬롯에서 제거한다
+					 * 5. 안남아있다면 잘 모르겠지만 GPU는 jc부터 가져가서 바로 수행하고 수행에 들어가면 next를 없애는 모양이다.
+					 * 6. 
+					 */
+					spin_lock_irqsave(&slot_atom->run_lock, run_flags);
+					katom->sched_stat |= SCHED_STAT_PCER;
+
+					if(slot_atom->run_lock_flags == 0){
+						slot_atom->run_lock_flags++;
+
+						dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+						slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;
+
+						dequeued_katom->sched_stat &= ~SCHED_STAT_RUN;
+						dequeued_katom->sched_stat |= SCHED_STAT_PSLOT;
+
+						kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+
+					}else{
+						slot_atom->run_lock_flags--;
+
+						if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) == 0){
+							//진짜로 동작 중일 경우
+
+							slot_atom->sched_stat |= SCHED_STAT_PRUN;
+							slot_atom->kctx->tInfo.nr_preempted++;
+						
+
+#ifdef _TSK_TRACE_EVICTION_
+							kbdev->run_evict++;
+							kbdev->eviction_time = ktime_get();
+#endif
+							
+							kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, JSn_COMMAND_HARD_STOP, slot_atom->core_req, slot_atom);
+
+							dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+
+							slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;
+							preempt_slot->submitted[(preempt_slot->submitted_head + preempt_slot->submitted_nr) & 15] = dequeued_katom;
+							preempt_slot->submitted_nr++;
+
+						}else{
+							kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
+
+							if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || 
+									kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0){
+
+								dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+								slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;
+
+								kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), 0, NULL);
+								kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), 0, NULL);
+
+								dequeued_katom->sched_stat &= ~SCHED_STAT_RUN;
+								dequeued_katom->sched_stat |= SCHED_STAT_PSLOT;
+
+								if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION)) 
+									slot->job_chain_flag = !slot->job_chain_flag;
+
+								kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+							}else{
+								//이 과정 수행도중에 job이 수행되버렸을 때
+								
+								slot_atom->sched_stat |= SCHED_STAT_PRUN;
+
+								kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, JSn_COMMAND_HARD_STOP, slot_atom->core_req, slot_atom);
+
+								dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+								slot->submitted[(slot->submitted_head + slot->submitted_nr+1) & BASE_JM_SUBMIT_SLOTS_MASK]=NULL;        
+								preempt_slot->submitted[(preempt_slot->submitted_head + preempt_slot->submitted_nr) & 15] = dequeued_katom;
+								preempt_slot->submitted_nr++;
+
+							}
+						}
+					}
+
+					spin_unlock_irqrestore(&slot_atom->run_lock, run_flags);
+#ifdef _TSK_TRACE_EVICTION_
+					katom->eviction_stat = 3;
+#endif
+					break;
+				}
+				spin_unlock_irqrestore(&sctx->sched_lock, sched_flags);
+			}	
+		
+		}else{//slot이 비어있는 경우
+			preempt_mode |= (1<<i);
+		}
+	}
+
+#ifdef _TSK_TRACE_EVICTION_
+	katom->p_delay = ktime_sub(ktime_get(),p_delay);
+	katom->launch_time = ktime_get();
+#endif
+
+	return preempt_mode;
+}
+
+u8 sc_resched(struct kbase_jd_atom *katom){
+
+	bool_t need_to_try_schedule_context = false;
+	struct kbase_context *kctx = katom->kctx;
+
+	if(katom->sched_stat & SCHED_STAT_PMASK){
+
+		if(katom->sched_stat & SCHED_STAT_PRUN)
+			katom->sched_stat |= SCHED_STAT_RERUN;
+
+		katom->event_code = BASE_JD_EVENT_DONE;
+		katom->status = KBASE_JD_ATOM_STATE_IN_JS;
+		need_to_try_schedule_context |= kbasep_js_add_job(kctx, katom);
+
+	}else{
+		need_to_try_schedule_context = jd_done_nolock(katom);
+	}
+
+	return need_to_try_schedule_context;
+}
+#endif
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_sched.h transcl/drivers/gpu/arm/midgard/mali_custom_sched.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_sched.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_sched.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,53 @@
+#define LIMIT_SUBMITTED_NR (2)
+
+#define SCHED_RT_PRIORITY (-19)
+
+//stat
+#define SCHED_STAT_INIT (0UL)     //(0<<0)
+#define SCHED_STAT_POOL (1UL)     //(1<<0)
+#define SCHED_STAT_SLOT (2UL)     //(1<<1)
+#define SCHED_STAT_RMASK (3UL)    //((1<<0)|(1<<1))=(SCHED_STAT_POOL|SCHED_STAT_SLOT)
+
+
+#define SCHED_STAT_SNAP (4UL)     //(1<<2)
+#define SCHED_STAT_RUN  (8UL)     //(1<<3)
+#define SCHED_STAT_DONE (16UL)    //(1<<4)
+#define SCHED_STAT_RERUN (32UL)   //(1<<5)
+#define SCHED_STAT_SMASK (12UL)   //((1<<2)|(1<<3))=(|SCHED_STAT_SNAP|SCHED_STAT_RUN)
+#define SCHED_STAT_SLOT_READY (0UL)
+//#define SCHED_STAT_RMASK (28)   //((1<<2)|(1<<3)|(1<<4))=(SCHED_STAT_SNAP|SCHED_STAT_RUN|SCHED_STAT_DONE)
+
+#define SCHED_STAT_PSLOT (64UL)   //(1<<6)
+#define SCHED_STAT_PSNAP (128UL)  //(1<<7)
+#define SCHED_STAT_PRUN (256UL)   //(1<<8)
+#define SCHED_STAT_PMASK (448UL)  //((1<<6)|(1<<7)|(1<<8))=(SCHED_STAT_PSLOT|SCHED_STAT_PSNAP|SCHED_STAT_PRUN)
+
+#define SCHED_STAT_PCER (512UL)  //(1<<9)
+#define SCHED_STAT_PSCED (1024UL) //(1<<10)
+#define SCHED_STAT_WAKE (2048UL) //(1<<11)
+
+//bit
+#define SCHED_STAT_POOL_BIT (0)
+#define SCHED_STAT_SLOT_BIT (1)
+
+#define SCHED_STAT_SNAP_BIT (2)
+#define SCHED_STAT_RUN_BIT  (3)
+#define SCHED_STAT_DONE_BIT (4)
+#define SCHED_STAT_RERUN_BIT (5)
+
+#define SCHED_STAT_PSLOT_BIT (6)
+#define SCHED_STAT_PSNAP_BIT (7)
+#define SCHED_STAT_PRUN_BIT (8)
+
+#define SCHED_STAT_PSCER_BIT (9)
+#define SCHED_STAT_PSCED_BIT (10)
+#define SCHED_STAT_WAKE_BIT (11)
+
+
+
+void sc_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js);
+int sc_preempt(struct kbase_jd_atom *katom);
+u32 sc_active_jobs(kbase_jm_slot *slot);
+u8 sc_is_preempt(kbase_jm_slot *slot);
+u8 sc_resched(struct kbase_jd_atom *katom);
+
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_snap.c transcl/drivers/gpu/arm/midgard/mali_custom_snap.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_snap.c	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_snap.c	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,288 @@
+#include <mali_kbase.h>     
+#include <mali_kbase_defs.h>
+#include <mali_kbase_mem.h>
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <mali_kbase_jm.h>
+#include <linux/sched.h>
+
+
+#ifdef _TSK_CUSTOM_SNAP_
+/*start*/
+
+void * memcpy_neon(void *, const void *, size_t);
+
+static INLINE int is_snapshot_cancel(struct kbase_jd_atom *katom){
+	
+	//if(test_bit(SCHED_STAT_PSCED_BIT,&katom->sched_stat)){// & SCHED_STAT_PSCED){
+	if(katom->sched_stat & SCHED_STAT_PSCED){
+		return 1;
+	}
+	return 0;
+}
+
+
+void create_snapthread(struct kbase_device *kbdev, int id){
+	struct sched_param param = {.sched_priority = 1};
+	kbdev->snapshot_ctx.kthread_ctx[id].snapshot_kthread = kthread_create(snapshot_kthread,(void*)&kbdev->snapshot_ctx.kthread_ctx[id],"snapshot_kthread");
+	sched_setscheduler(kbdev->snapshot_ctx.kthread_ctx[id].snapshot_kthread, SCHED_FIFO, &param);
+	kthread_bind(kbdev->snapshot_ctx.kthread_ctx[id].snapshot_kthread, KTHREAD_CORE_BASE+id);
+	wake_up_process(kbdev->snapshot_ctx.kthread_ctx[id].snapshot_kthread);
+}
+
+void destroy_snapthread(struct kbase_device *kbdev, int id){
+	kthread_stop(kbdev->snapshot_ctx.kthread_ctx[id].snapshot_kthread);
+}   
+
+void init_snapshot_ctx(struct kbase_device *kbdev){                                  
+	int i, j;
+	kbdev->snapshot_ctx.skthread_head = 0;
+	for(i=0;i<NR_SNAPSHOT_KTHREAD;i++){
+
+		kbdev->snapshot_ctx.kthread_ctx[i].id = i;
+		kbdev->snapshot_ctx.kthread_ctx[i].kbdev = kbdev;
+		kbdev->snapshot_ctx.kthread_ctx[i].katom = NULL;
+		kbdev->snapshot_ctx.kthread_ctx[i].snapshot_kthread = NULL;
+		spin_lock_init(&kbdev->snapshot_ctx.kthread_ctx[i].sched_lock);
+		spin_lock_init(&kbdev->snapshot_ctx.kthread_ctx[i].snap_thread_lock);
+
+		kbdev->snapshot_ctx.kthread_ctx[i].curr_pos = 0;
+		kbdev->snapshot_ctx.kthread_ctx[i].head_pos = 0;
+		atomic_set(&kbdev->snapshot_ctx.kthread_ctx[i].nr_snap_atom,0);
+		for(j=0;j<10;j++)
+			kbdev->snapshot_ctx.kthread_ctx[i].snap_atom_list[j] = NULL;
+
+
+		init_waitqueue_head(&kbdev->snapshot_ctx.kthread_ctx[i].snap_wqueue);
+		create_snapthread(kbdev, i);
+	}
+				                                                                                     
+}
+
+void change_snapshot_thread(struct kbase_device *kbdev){
+
+	int i;
+	for(i=0;i<NR_SNAPSHOT_KTHREAD;i++){
+		if(!atomic_read(&kbdev->snapshot_ctx.kthread_ctx[i].nr_snap_atom)){
+			kbdev->snapshot_ctx.skthread_head = i;
+			return;
+		}
+	}
+	kbdev->snapshot_ctx.skthread_head = (kbdev->snapshot_ctx.skthread_head + 1) % NR_SNAPSHOT_KTHREAD;
+}
+
+void snapshot_ipi(void *snapshot_context){
+	unsigned long flags;
+	struct snapshot_kthread_context *sctx = (struct snapshot_kthread_context*)snapshot_context;
+	struct kbase_device *kbdev = sctx->kbdev;
+
+	int i, ret = 0;
+	
+	if(sctx->katom == NULL){
+		//trace_gpu_custom("kthread - wakeup null", ktime_to_ns(ktime_get()), 0, 0, (u32)0, (u32)0);
+		goto cancel;
+	}
+	//trace_gpu_custom("kthread - start", ktime_to_ns(ktime_get()), sctx->katom->kctx->ctx_id, sctx->katom->atom_id, (u32)0, (u32)0);
+	
+	if((ret = is_snapshot_cancel(sctx->katom)))
+		goto cancel;
+
+	if(!(sctx->katom->sched_stat & SCHED_STAT_RERUN))
+		snapshot_store(sctx->katom);
+	else{                           
+		snapshot_restore(sctx->katom);
+	}
+
+	spin_lock_irqsave(&sctx->sched_lock, flags);
+	if((ret = is_snapshot_cancel(sctx->katom))){
+		spin_unlock_irqrestore(&sctx->sched_lock, flags);
+		goto cancel;
+	}
+	
+	sctx->katom->sched_stat ^= SCHED_STAT_SMASK; //~SCHED_STAT_SNAP & SCHED_STAT_RUN
+	spin_unlock_irqrestore(&sctx->sched_lock, flags);
+
+	//trace_gpu_custom("kthread - submit call", ktime_to_ns(ktime_get()), sctx->katom->kctx->ctx_id, sctx->katom->atom_id, (u32)0, (u32)0);
+	sc_hw_submit(kbdev, sctx->katom, 1);
+
+	sctx->katom->param.nr_restore = 0;
+	for(i=sctx->katom->param.nr_restore;sctx->katom->param.param[i]!=NULL;i++)
+		sctx->katom->param.nr_restore_pages[i]=0;
+
+cancel:
+	if(ret){
+		atomic_dec(&sctx->katom->kctx->process_preempt);
+		clear_bit(SCHED_STAT_PSCED_BIT, &sctx->katom->sched_stat);
+#ifdef _TSK_TRACE_EVICTION_
+		if(kbdev->sw_trace == 2)
+			trace_gpu_custom_bench("SNAP-EVICTION",sctx->katom->atom_id, ktime_to_ns(ktime_sub(ktime_get(), sctx->katom->snapc_time)), 0,0,0,0,0,0,0,0);
+#endif
+	}
+	
+	//trace_gpu_custom("kthread - sleep-1", ktime_to_ns(ktime_get()), sctx->katom->kctx->ctx_id, sctx->katom->atom_id, (u32)0, (u32)0);
+	sctx->katom = NULL;
+	//trace_gpu_custom("kthread - exit", ktime_to_ns(ktime_get()), 0, 0, (u32)0, (u32)0);
+	return;
+}
+
+void submit_snapshot(int cpu, struct snapshot_kthread_context* sctx){
+
+	smp_call_function_single(cpu+4, snapshot_ipi, (void*)sctx, 0);
+}
+
+int snapshot_kthread(void *param){
+	
+	unsigned long flags;
+	struct snapshot_kthread_context *sctx = (struct snapshot_kthread_context*)param;
+	struct kbase_device *kbdev = sctx->kbdev;
+	struct kbase_jd_atom *katom = NULL;
+
+	int i, ret = 0;
+
+	do{
+		if(!atomic_read(&sctx->nr_snap_atom)){
+			interruptible_sleep_on(&sctx->snap_wqueue);
+		}
+
+		katom = sctx->snap_atom_list[sctx->curr_pos];
+
+		if(katom == NULL){
+			//trace_gpu_custom("kthread - wakeup null", ktime_to_ns(ktime_get()), 0, 0, (u32)sctx->curr_pos, (u32)atomic_read(&sctx->nr_snap_atom));
+			goto sleep;
+		}
+		//trace_gpu_custom("kthread - start", ktime_to_ns(ktime_get()), katom->kctx->tInfo.ctx_id, katom->tInfo.atom_id, (u32)katom->sched_stat, (u32)sctx->id);
+		
+		//snapshot
+		if((ret = is_snapshot_cancel(katom)))
+			goto sleep;
+
+		if(!(katom->sched_stat & SCHED_STAT_RERUN)){
+			snapshot_store(katom);
+		}else{                           
+			snapshot_restore(katom);
+		}
+		
+		spin_lock_irqsave(&sctx->sched_lock, flags);
+		if((ret = is_snapshot_cancel(katom))){
+			spin_unlock_irqrestore(&sctx->sched_lock, flags);
+			goto sleep;
+		}
+
+
+		katom->sched_stat ^= SCHED_STAT_SMASK; //~SCHED_STAT_SNAP & SCHED_STAT_RUN
+		spin_unlock_irqrestore(&sctx->sched_lock, flags);
+
+		sc_hw_submit(kbdev, katom, 1);
+		//kbase_job_hw_submit(kbdev, sctx->katom, 1);
+
+		katom->param.nr_restore = 0;
+		for(i=katom->param.nr_restore;katom->param.param[i]!=NULL;i++)
+			katom->param.nr_restore_pages[i]=0;
+
+
+sleep:
+		if(ret){
+			ret = 0;
+			atomic_dec(&katom->kctx->process_preempt);
+			clear_bit(SCHED_STAT_PSCED_BIT, &katom->sched_stat);
+#ifdef _TSK_TRACE_EVICTION_
+			if(kbdev->sw_trace == 2)
+				trace_gpu_custom_bench("SNAP-EVICTION",katom->atom_id, ktime_to_ns(ktime_sub(ktime_get(), katom->snapc_time)),0,0,0,0,0,0,0,0);
+#endif
+		}
+	
+		katom = NULL;	
+		sctx->snap_atom_list[sctx->curr_pos] = NULL;
+		atomic_dec(&sctx->nr_snap_atom);
+		sctx->curr_pos = (sctx->curr_pos+1)%10;
+		
+	}while(1);
+
+	return 0;
+}
+
+/*end*/
+
+void snapshot_store(struct kbase_jd_atom* katom){
+	int i, j;
+	struct kbase_va_region* reg = NULL;
+	//unsigned long mem_flag;
+	
+	u32 granularity = katom->kctx->kbdev->snap_granularity;
+	int limit_pages_granularity;
+
+	//kbase_gpu_vm_lock(katom->kctx);
+	//spin_lock_irqsave(&katom->kctx->snap_lock, mem_flag);
+#ifdef _TSK_CUSTOM_TRACE_
+	job_trace_snapshot_start(katom);
+#endif
+
+	for(i=katom->param.nr_store;katom->param.param[i]!=NULL;i++){
+		        reg = katom->param.param[i];
+			limit_pages_granularity = reg->dreg.nr_pages/granularity;
+
+			for(j=katom->param.nr_store_pages[i];j<limit_pages_granularity;j++){
+				if(is_snapshot_cancel(katom))
+					goto snapshot_cancel;
+
+				memcpy_neon(reg->dreg.kdp+j*PAGE_SIZE*granularity, reg->dreg.ksp+j*PAGE_SIZE*granularity, PAGE_SIZE*granularity);
+				katom->param.nr_store_pages[i]++;
+			}
+
+			if((reg->dreg.nr_pages%granularity != 0) && (katom->param.nr_store_pages[i] < limit_pages_granularity+1)){
+				memcpy_neon(reg->dreg.kdp+j*PAGE_SIZE*granularity, reg->dreg.ksp+j*PAGE_SIZE*granularity, PAGE_SIZE*(reg->dreg.nr_pages%granularity));
+				katom->param.nr_store_pages[i]++;
+			}
+
+			
+			katom->param.nr_store++;
+#ifdef _TSK_CUSTOM_TRACE_
+			job_trace_snapshot_pages(katom, reg->dreg.nr_pages);
+#endif
+	}
+snapshot_cancel:
+#ifdef _TSK_CUSTOM_TRACE_
+	job_trace_snapshot_end(katom);
+#endif
+	return;
+        //spin_unlock_irqrestore(&katom->kctx->snap_lock, mem_flag);
+	//kbase_gpu_vm_unlock(katom->kctx);
+}
+
+void snapshot_restore(struct kbase_jd_atom* katom){
+	int i, j;
+	struct kbase_va_region* reg;
+	//unsigned long mem_flag;
+	
+	u32 granularity = katom->kctx->kbdev->snap_granularity;
+	int limit_pages_granularity;                           
+
+	//kbase_gpu_vm_lock(katom->kctx);
+	//spin_lock_irqsave(&katom->kctx->snap_lock, mem_flag);
+	reg = NULL;
+
+	for(i=katom->param.nr_restore;katom->param.param[i]!=NULL;i++){
+			reg = katom->param.param[i];
+			limit_pages_granularity = reg->dreg.nr_pages/granularity;
+
+			for(j=katom->param.nr_restore_pages[i];j<limit_pages_granularity;j++){
+				if(is_snapshot_cancel(katom))
+					goto snapshot_cancel;
+
+				memcpy_neon(reg->dreg.ksp+j*PAGE_SIZE*granularity, reg->dreg.kdp+j*PAGE_SIZE*granularity, PAGE_SIZE*granularity);
+				katom->param.nr_restore_pages[i]++;
+			}
+
+			if((reg->dreg.nr_pages%granularity != 0) && (katom->param.nr_restore_pages[i] < limit_pages_granularity+1)){
+				memcpy_neon(reg->dreg.ksp+j*PAGE_SIZE*granularity, reg->dreg.kdp+j*PAGE_SIZE*granularity, PAGE_SIZE*(reg->dreg.nr_pages%granularity));
+				katom->param.nr_restore_pages[i]++;
+			}
+
+			katom->param.nr_restore++;
+	}
+snapshot_cancel:
+	return;
+	//spin_unlock_irqrestore(&katom->kctx->snap_lock, mem_flag);
+	//kbase_gpu_vm_unlock(katom->kctx);
+}
+#endif
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_snap.h transcl/drivers/gpu/arm/midgard/mali_custom_snap.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_snap.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_snap.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,60 @@
+#define KTHREAD_CORE_BASE (4)
+#define NR_SNAPSHOT_KTHREAD (4)
+#define BASE_SNAP_GRANULARITY (1280)
+
+typedef struct kernel_live_context_param{
+    struct list_head param_node;
+    struct kbase_va_region* param[20];
+    u32 nr_store, nr_restore;
+    u32 nr_store_pages[20];
+    u32 nr_restore_pages[20];
+}kernel_lparam;
+
+
+typedef struct custom_dump_region{
+    struct list_head dreg_node;
+
+    struct kbase_context *kctx;
+    struct kbase_va_region *reg;
+        
+    size_t nr_pages;
+    struct page **sp;
+    struct page **dp;
+    void *ksp;
+    void *kdp;
+    u8 is_vmalloc;
+
+}custom_dump_region;
+
+typedef struct snapshot_kthread_context{
+
+    int id;
+    struct kbase_device *kbdev;
+    struct kbase_jd_atom *katom;
+    struct task_struct *snapshot_kthread;
+    wait_queue_head_t snap_wqueue;
+
+    u8 curr_pos, head_pos;
+    atomic_t nr_snap_atom;
+    struct kbase_jd_atom *snap_atom_list[10];
+
+	spinlock_t sched_lock, snap_thread_lock;
+
+}snapshot_kthread_context;
+
+typedef struct snapshot_kthreads{
+    u32 skthread_head;
+    snapshot_kthread_context kthread_ctx[NR_SNAPSHOT_KTHREAD];
+
+}snapshot_kthreads;
+
+
+void init_snapshot_ctx(struct kbase_device *kbdev);
+void change_snapshot_thread(struct kbase_device *kbdev);
+
+void submit_snapshot(int cpu, struct snapshot_kthread_context* sctx);
+int snapshot_kthread(void *param);
+void create_snapthread(struct kbase_device *kbdev, int id);
+void destroy_snapthread(struct kbase_device *kbdev,int id);
+void snapshot_store(struct kbase_jd_atom* katom);
+void snapshot_restore(struct kbase_jd_atom* katom);
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_tsk.h transcl/drivers/gpu/arm/midgard/mali_custom_tsk.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_custom_tsk.h	1970-01-01 09:00:00.000000000 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_custom_tsk.h	2018-02-21 15:57:49.496649820 +0900
@@ -0,0 +1,36 @@
+#define _TSK_CUSTOM_PRINT_
+
+/*
+#define _TSK_TRACE_EVAL_
+//#define _TSK_TRACE_PER_KERNEL_
+//#define _TSK_TRACE_EVICTION_
+*/
+
+#ifdef CONFIG_GPU_TRACEPOINTS
+
+#define _TSK_CUSTOM_TRACE_
+#ifdef _TSK_CUSTOM_TRACE_
+#include <mali_custom_eval.h>
+#endif
+
+#endif
+
+#define _TSK_CUSTOM_SNAP_
+#ifdef _TSK_CUSTOM_SNAP_
+#include <mali_custom_snap.h>
+#endif
+
+#define _TSK_CUSTOM_SCHED_
+#ifdef _TSK_CUSTOM_SCHED_
+#include <mali_custom_sched.h>
+#endif
+
+#define _TSK_CUSTOM_PROC_
+#ifdef _TSK_CUSTOM_PROC_
+#include <mali_custom_proc.h>
+#endif
+
+#define _TSK_CUSTOM_IOCTL_
+#ifdef _TSK_CUSTOM_IOCTL_ 
+#include <mali_custom_ioctl.h>
+#endif
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_context.c transcl/drivers/gpu/arm/midgard/mali_kbase_context.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_context.c	2018-02-21 16:00:25.637619404 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_context.c	2018-02-21 15:57:49.497649813 +0900
@@ -108,11 +108,22 @@ kbase_context *kbase_create_context(kbas
 #ifdef CONFIG_MALI_TRACE_TIMELINE
 	atomic_set(&kctx->timeline.jd_atoms_in_flight, 0);
 #endif
-	//measure
-	kctx->app_start = ktime_get();
-	kctx->mem_count = 0;
-	kctx->kernel_count = 0;
 
+#ifdef _TSK_CUSTOM_TRACE_
+	context_trace_init(kctx);
+#endif
+
+#ifdef _TSK_CUSTOM_SCHED_
+	atomic_set(&kctx->process_preempt, 0);
+	kctx->nr_dep_job = 0;
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+	spin_lock_init(&kctx->snap_lock);
+#endif
+#ifdef _TSK_CUSTOM_PROC_ 
+	INIT_LIST_HEAD(&kctx->proc_ctx_list_node);
+	list_add_tail(&kctx->proc_ctx_list_node, &kbdev->proc_ctx_list);
+#endif
 
 	return kctx;
 
@@ -163,17 +174,15 @@ void kbase_destroy_context(kbase_context
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
 
 	KBASE_TRACE_ADD(kbdev, CORE_CTX_DESTROY, kctx, NULL, 0u, 0u);
+#ifdef _TSK_CUSTOM_TRACE_
+	context_trace_release(kctx);
+	printout_context_trace(kctx);
+#endif
 
+#ifdef _TSK_CUSTOM_PROC_
+	list_del(&kctx->proc_ctx_list_node);
+#endif
 
-	//measure
-	kctx->app_end = ktime_get();
-#ifdef CONFIG_GPU_TRACEPOINTS
-	trace_gpu_custom("Context Destroy", ktime_to_ns(ktime_sub(kctx->app_end, kctx->app_start)), 0, 0, (u32)kctx->mem_count, (u32)kctx->kernel_count);
-#endif
-/*	printk(KERN_ALERT"Application GPU Using Time : %llu\n", ktime_to_us(kctx->app_end) - ktime_to_us(kctx->app_start));
-	printk(KERN_ALERT"Application GPU Using Memory : %d\n", kctx->mem_count);
-	printk(KERN_ALERT"Application GPU Submitting Kernel : %d\n", kctx->kernel_count);
-*/
 	/* Ensure the core is powered up for the destroy process */
 	/* A suspend won't happen here, because we're in a syscall from a userspace
 	 * thread. */
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_core_linux.c transcl/drivers/gpu/arm/midgard/mali_kbase_core_linux.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_core_linux.c	2018-02-21 16:00:25.638619398 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_core_linux.c	2018-02-21 15:57:49.497649813 +0900
@@ -950,6 +950,11 @@ static long kbase_ioctl(struct file *fil
 	u32 size = _IOC_SIZE(cmd);
 	kbase_context *kctx = filp->private_data;
 
+#ifdef _TSK_CUSTOM_IOCTL_                          
+	if(_IOC_TYPE(cmd)==100)
+		kbase_custom_ioctl(kctx, cmd, arg);
+#endif
+
 	if (size > CALL_MAX_SIZE)
 		return -ENOTTY;
 
@@ -1075,6 +1080,22 @@ static irqreturn_t kbase_job_irq_handler
 	unsigned long flags;
 	struct kbase_device *kbdev = kbase_untag(data);
 	u32 val;
+#ifdef _TSK_CUSTOM_SCHED_
+	val = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS), NULL);
+
+	if(kbdev->run_evict && val >> 16 ){
+		kbdev->run_evict--;
+		kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), val & ((1 << 1) | (1 << (1 + 16))), NULL);
+
+#ifdef _TSK_TRACE_EVICTION_
+		if(kbdev->sw_trace == 1){
+			trace_gpu_custom_bench("RUN-EVICTION", kbdev->run_evict, ktime_to_ns(ktime_sub(ktime_get(), kbdev->eviction_time)),0,0,0,0,0,0,0,0);
+		}
+#endif
+	}
+
+#endif
+
 
 	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
 
@@ -1083,8 +1104,9 @@ static irqreturn_t kbase_job_irq_handler
 		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
 		return IRQ_NONE;
 	}
-
+#ifndef _TSK_CUSTOM_SCHED_
 	val = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS), NULL);
+#endif
 
 #ifdef CONFIG_MALI_DEBUG
 	if (!kbdev->pm.driver_ready_for_irqs)
@@ -1098,6 +1120,10 @@ static irqreturn_t kbase_job_irq_handler
 
 	KBASE_LOG(3, kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
 
+#ifdef _TSK_CUSTOM_TRACE_
+	job_trace_run_int_end(kbdev);
+#endif
+
 	kbase_job_done(kbdev, val);
 
 	return IRQ_HANDLED;
@@ -2368,6 +2394,17 @@ static int kbase_common_device_init(kbas
 
 	int inited = 0;
 
+#ifdef _TSK_CUSTOM_TRACE_
+	device_trace_init(kbdev);
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+	init_snapshot_ctx(kbdev);
+	kbdev->snap_granularity = BASE_SNAP_GRANULARITY;//5MB
+#endif
+#ifdef _TSK_CUSTOM_PROC_
+	kbase_mem_proc_init(kbdev);
+#endif
+
 	dev_set_drvdata(kbdev->dev, kbdev);
 
 	kbdev->mdev.minor = MISC_DYNAMIC_MINOR;
@@ -2810,6 +2847,10 @@ static int kbase_platform_device_remove(
 	if (!kbdev)
 		return -ENODEV;
 
+#ifdef _TSK_CUSTOM_PROC_
+	kbase_mem_proc_exit();
+#endif
+
 	return kbase_common_device_remove(kbdev);
 }
 
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_defs.h transcl/drivers/gpu/arm/midgard/mali_kbase_defs.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_defs.h	2018-02-21 16:00:25.638619398 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_defs.h	2018-02-21 15:57:49.497649813 +0900
@@ -47,12 +47,12 @@
 
 /** Enable SW tracing when set */
 #ifdef CONFIG_MALI_MIDGARD_ENABLE_TRACE
-#define KBASE_TRACE_ENABLE 1
+#define KBASE_TRACE_ENABLE 0
 #endif
 
 #ifndef KBASE_TRACE_ENABLE
 #ifdef CONFIG_MALI_DEBUG
-#define KBASE_TRACE_ENABLE 1
+#define KBASE_TRACE_ENABLE 0
 #else
 #define KBASE_TRACE_ENABLE 0
 #endif				/* CONFIG_MALI_DEBUG */
@@ -217,13 +217,43 @@ typedef enum {
 
 typedef struct kbase_jd_atom kbase_jd_atom;
 
+#define _TSK_CUSTOM_ 
+#ifdef _TSK_CUSTOM_
+typedef struct kbase_jm_slot kbase_jm_slot;
+#include <mali_custom_tsk.h>
+#endif
+
 struct kbase_ext_res
 {
 	mali_addr64 gpu_address;
 	struct kbase_mem_phy_alloc * alloc;
 };
 
+
 struct kbase_jd_atom {
+#ifdef _TSK_CUSTOM_TRACE_
+    job_trace tInfo;
+#endif
+#ifdef _TSK_CUSTOM_SCHED_
+    unsigned long  sched_stat;
+    spinlock_t run_lock;
+    u32 run_lock_flags;
+    u8 dep_flag;
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+    u8 nr_param;
+    struct list_head param_list;//확장용도
+    kernel_lparam param;
+    u64 jc_indirect;
+#endif
+#ifdef _TSK_TRACE_EVICTION_
+    u8 eviction_stat;
+    ktime_t p_delay;
+    ktime_t launch_time;
+    u8 snapc_stat;
+    ktime_t snapc_time;
+    u64 patom[2];
+#endif
 	struct work_struct work;
 	ktime_t start_timestamp;
 
@@ -342,6 +372,9 @@ typedef struct kbase_jm_slot {
 #define BASE_JM_SUBMIT_SLOTS        16
 #define BASE_JM_SUBMIT_SLOTS_MASK   (BASE_JM_SUBMIT_SLOTS - 1)
 
+#ifdef _TSK_CUSTOM_SCHED_
+    struct kbase_device *kbdev;
+#endif
 	struct kbase_jd_atom *submitted[BASE_JM_SUBMIT_SLOTS];
 
 	kbase_context *last_context;
@@ -562,6 +595,9 @@ typedef struct kbasep_kctx_list_element
 struct kbase_device {
 	/** jm_slots is protected by kbasep_js_device_data::runpool_irq::lock */
 	kbase_jm_slot jm_slots[BASE_JM_MAX_NR_SLOTS];
+#ifdef _TSK_CUSTOM_SCHED_
+	kbase_jm_slot preempt_slot;
+#endif
 	s8 slot_submit_count_irq[BASE_JM_MAX_NR_SLOTS];
 
 	struct list_head entry;
@@ -767,7 +803,25 @@ struct kbase_device {
 	 */
 	mali_bool force_replay_random;
 #endif
-    ktime_t kernel_start, kernel_end;
+
+#ifdef _TSK_CUSTOM_TRACE_
+    device_trace tInfo;
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+    struct snapshot_kthreads snapshot_ctx;
+    u32 snap_granularity;
+#endif
+#ifdef _TSK_CUSTOM_PROC_
+    struct list_head proc_ctx_list;
+#endif
+#ifdef _TSK_CUSTOM_SCHED_
+    u8 run_evict;
+#endif
+#ifdef _TSK_TRACE_EVICTION_
+    u8 sw_trace;
+    //for running job eviction
+    ktime_t eviction_time;
+#endif
 };
 
 struct kbase_context {
@@ -835,11 +889,20 @@ struct kbase_context {
 #ifdef CONFIG_MALI_TRACE_TIMELINE
 	kbase_trace_kctx_timeline timeline;
 #endif
-    //measure
-    ktime_t app_start, app_end;
-    int mem_count;
-    int kernel_count;
 
+#ifdef _TSK_CUSTOM_SCHED_
+    atomic_t process_preempt;
+    int nr_dep_job;
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+    spinlock_t snap_lock;
+#endif 
+#ifdef _TSK_CUSTOM_PROC_                
+    struct list_head proc_ctx_list_node;
+#endif
+#ifdef _TSK_CUSTOM_TRACE_
+    context_trace tInfo;
+#endif
 };
 
 typedef enum kbase_reg_access_type {
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_device.c transcl/drivers/gpu/arm/midgard/mali_kbase_device.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_device.c	2018-02-21 16:00:25.638619398 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_device.c	2018-02-21 15:57:49.497649813 +0900
@@ -369,14 +369,22 @@ void kbase_gpu_interrupt(kbase_device *k
 	if (val & GPU_FAULT)
 		kbase_report_gpu_fault(kbdev, val & MULTIPLE_GPU_FAULTS);
 
-	if (val & RESET_COMPLETED)
+	if (val & RESET_COMPLETED){
+#ifdef CONFIG_GPU_TRACEPOINTS                                                                                                
+		//trace_gpu_custom("gpu reset completed", ktime_to_ns(ktime_get()), 0, 0, (u32)0, (u32)0);
+#endif
 		kbase_pm_reset_done(kbdev);
+	}
 
 	if (val & PRFCNT_SAMPLE_COMPLETED)
 		kbase_instr_hwcnt_sample_done(kbdev);
 
-	if (val & CLEAN_CACHES_COMPLETED)
+	if (val & CLEAN_CACHES_COMPLETED){
+#ifdef CONFIG_GPU_TRACEPOINTS                                                                           
+		//trace_gpu_custom("gpu cache clean completed", ktime_to_ns(ktime_get()), 0, 0, (u32)0, (u32)0);
+#endif
 		kbase_clean_caches_done(kbdev);
+	}
 
 	KBASE_TRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, NULL, 0u, val);
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val, NULL);
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jd.c transcl/drivers/gpu/arm/midgard/mali_kbase_jd.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jd.c	2018-02-21 16:00:25.639619391 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_jd.c	2018-02-21 15:57:49.498649807 +0900
@@ -70,7 +70,7 @@ static int jd_run_atom(kbase_jd_atom *ka
 {
 	kbase_context *kctx = katom->kctx;
 	KBASE_DEBUG_ASSERT(katom->status != KBASE_JD_ATOM_STATE_UNUSED);
-
+	        //if(!(katom->core_req & BASE_JD_REQ_SOFT_JOB)))
 	if ((katom->core_req & BASEP_JD_REQ_ATOM_TYPE) == BASE_JD_REQ_DEP) {
 		/* Dependency only atom */
 		katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
@@ -785,6 +785,7 @@ static const char * const core_req_strin
 	"Vertex/Geometry Shader Job + Tiler Job",
 	"Unknown Job"
 };
+/*
 static const char *kbasep_map_core_reqs_to_string(base_jd_core_req core_req)
 {
 	if (core_req & BASE_JD_REQ_SOFT_JOB)
@@ -810,7 +811,7 @@ static const char *kbasep_map_core_reqs_
 		return core_req_strings[CORE_REQ_FRAGMENT_VERTEX_TILER];
 	}
 	return core_req_strings[CORE_REQ_UNKNOWN];
-}
+}*/
 #endif
 
 mali_bool jd_submit_atom(kbase_context *kctx,
@@ -822,6 +823,20 @@ mali_bool jd_submit_atom(kbase_context *
 	int queued = 0;
 	int i;
 	mali_bool ret;
+#ifdef _TSK_CUSTOM_SNAP_
+	u32 start, init_i, s_i;
+	u32* klc_info;
+	u32* param_end_info;
+	u32 nr_sparam = 0;
+	u64* tmp_sparam;
+	struct kbase_va_region* tmp_sreg;
+#endif
+#ifdef _TSK_CUSTOM_SCHED_
+	katom->sched_stat = SCHED_STAT_INIT;
+	katom->dep_flag = 0;
+	katom->run_lock_flags = 0;
+	spin_lock_init(&katom->run_lock);
+#endif
 
 	/* Update the TOTAL number of jobs. This includes those not tracked by
 	 * the scheduler: 'not ready to run' and 'dependency-only' jobs. */
@@ -841,6 +856,74 @@ mali_bool jd_submit_atom(kbase_context *
 	katom->nice_prio = user_atom->prio;
 	katom->atom_flags = 0;
 	katom->retry_count = 0;
+#ifdef _TSK_CUSTOM_SNAP_
+	
+	katom->nr_param = 0;
+	INIT_LIST_HEAD(&katom->param_list);
+
+	//rt_skip
+	if(kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority > SCHED_RT_PRIORITY 
+			&& !(katom->core_req & BASE_JD_REQ_SOFT_JOB)){
+
+		katom->param.nr_store = 0;
+		katom->param.nr_restore = 0;
+
+		list_add(&katom->param.param_node, &katom->param_list);
+		for(init_i=0;init_i<20;init_i++){
+			katom->param.param[init_i] = NULL;
+			katom->param.nr_store_pages[init_i]=0;
+			katom->param.nr_restore_pages[init_i]=0;
+		}
+
+	
+		klc_info = (u32*)((u32)katom->jc);
+		katom->jc_indirect = katom->jc;
+
+		if( (*(klc_info+4) & 0x0000000f) == 0x00000005 ){
+			katom->jc_indirect = (u64)(*(klc_info+6));
+			klc_info = (u32*)(*(klc_info+6));
+		}
+
+		param_end_info = (u32*)(*(klc_info+20));
+		nr_sparam =(((*param_end_info & 0x000fff00) >> 8) - (*(klc_info+23) & 0x00000fff))/8;
+		tmp_sparam = (u64*)(*(klc_info+23));
+		
+		//printk(KERN_ALERT"ID[%llu](2) 0x%08x - 0x%08x - %u\n", 
+		//		katom->atom_id, (u32)klc_info, (u32)((*(klc_info+4) & 0xffff0000)>>16), (u32)(((*(klc_info+4) & 0xffff0000)>>16)-0x4000));
+
+
+
+		for(start = 0;start<nr_sparam;start++){
+			init_i =1;
+
+			tmp_sreg = kbase_region_tracker_find_region_enclosing_address(kctx, *(tmp_sparam+start));
+
+			if(tmp_sreg !=NULL){
+				if(tmp_sreg->tInfo.reg_id != 0){
+
+					for(s_i=0; katom->param.param[s_i] != NULL;s_i++){
+						if(katom->param.param[s_i] == tmp_sreg){
+							init_i = 0;
+							break;
+						}
+					}
+
+					if(init_i){
+						katom->param.param[s_i] = tmp_sreg;
+					}
+
+				}//reg_id != 0
+			}
+		}
+
+	}
+	/*printk(KERN_ALERT"atom : %llu\n",katom->atom_id);
+	for(s_i=0; katom->param.param[s_i] != NULL;s_i++){
+		printk(KERN_ALERT" rid : %u\n",katom->param.param[s_i]->reg_id);
+	}*/
+
+
+#endif
 #ifdef CONFIG_KDS
 	/* Start by assuming that the KDS dependencies are satisfied,
 	 * kbase_jd_pre_external_resources will correct this if there are dependencies */
@@ -887,9 +970,18 @@ mali_bool jd_submit_atom(kbase_context *
 				list_add_tail(&katom->dep_item[i], &dep_atom->dep_head[i]);
 				katom->dep_atom[i] = dep_atom;
 				queued = 1;
+#ifdef _TSK_CUSTOM_SCHED_
+				if((katom->core_req & BASE_JD_REQ_ONLY_COMPUTE)){
+					kctx->nr_dep_job++;
+					katom->dep_flag = 1;
+				}
+#endif
 			}
 		}
 	}
+#ifdef _TSK_CUSTOM_TRACE_
+	job_trace_init(katom);
+#endif
 
 	/* These must occur after the above loop to ensure that an atom that
 	 * depends on a previous atom with the same number behaves as expected */
@@ -1207,8 +1299,9 @@ static void jd_done_worker(struct work_s
 		kbasep_js_set_job_retry_submit_slot(katom, 1);
 	}
 
-	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316))
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316)){
 		kbase_as_poking_timer_release_atom(kbdev, kctx, katom);
+	}
 
 	/* Release cores this job was using (this might power down unused cores, and
 	 * cause extra latency if a job submitted here - such as depenedent jobs -
@@ -1253,7 +1346,11 @@ static void jd_done_worker(struct work_s
 		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 		/* jd_done_nolock() requires the jsctx_mutex lock to be dropped */
 
+#ifdef _TSK_CUSTOM_SCHED_
+		need_to_try_schedule_context = sc_resched(katom);
+#else
 		need_to_try_schedule_context = jd_done_nolock(katom);
+#endif
 
 		/* This ctx is already scheduled in, so return value guarenteed FALSE */
 		KBASE_DEBUG_ASSERT(need_to_try_schedule_context == MALI_FALSE);
@@ -1371,7 +1468,8 @@ void kbase_jd_done(kbase_jd_atom *katom,
 
 	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
 	INIT_WORK(&katom->work, jd_done_worker);
-	queue_work(kctx->jctx.job_done_wq, &katom->work);
+	//queue_work(kctx->jctx.job_done_wq, &katom->work);
+	queue_work_on(7,kctx->jctx.job_done_wq, &katom->work);
 }
 
 KBASE_EXPORT_TEST_API(kbase_jd_done)
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jm.c transcl/drivers/gpu/arm/midgard/mali_kbase_jm.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jm.c	2018-02-21 16:00:25.639619391 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_jm.c	2018-02-21 15:57:49.498649807 +0900
@@ -42,14 +42,18 @@ u64 mali_js2_affinity_mask = 0xFFFFFFFFF
 static void kbasep_try_reset_gpu_early(kbase_device *kbdev);
 
 #ifdef CONFIG_GPU_TRACEPOINTS
-static char *kbasep_make_job_slot_string(int js, char *js_string)
+/*static char *kbasep_make_job_slot_string(int js, char *js_string)
 {
 	sprintf(js_string, "job_slot_%i", js);
 	return js_string;
-}
+}*/
 #endif
 
+#ifdef _TSK_CUSTOM_SCHED_ //우리 커널에서는 안쓰임 -> sc_hw_submit 으로 대체
+void kbase_job_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js)
+#else
 static void kbase_job_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js)
+#endif
 {
 	kbase_context *kctx;
 	u32 cfg;
@@ -130,20 +134,17 @@ static void kbase_job_hw_submit(kbase_de
 	kbase_trace_mali_job_slots_event(GATOR_MAKE_EVENT(GATOR_JOB_SLOT_START, js), kctx, kbase_jd_atom_id(kctx, katom)); 
 #endif				/* CONFIG_MALI_GATOR_SUPPORT */
 #ifdef CONFIG_GPU_TRACEPOINTS
-	if (kbasep_jm_nr_jobs_submitted(&kbdev->jm_slots[js]) == 1)
-	{
+	//if (kbasep_jm_nr_jobs_submitted(&kbdev->jm_slots[js]) == 1)
+	//{
 		/* If this is the only job on the slot, trace it as starting */
-		char js_string[16];
+		//char js_string[16];
 		//trace_gpu_sched_switch(kbasep_make_job_slot_string(js, js_string), ktime_to_ns(katom->start_timestamp), (u32)katom->kctx, 0, katom->work_id);
-		kbdev->jm_slots[js].last_context = katom->kctx;
-	}
+	//	kbdev->jm_slots[js].last_context = katom->kctx;
+	//}
 #endif
 	kbase_timeline_job_slot_submit(kbdev, kctx, katom, js);
 
 	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_START, katom->kctx);
-	katom->kctx->kernel_count++;
-	kbdev->kernel_start = ktime_get();
-	//printk(KERN_ALERT"submit kernel\n");
 }
 
 void kbase_job_submit_nolock(kbase_device *kbdev, kbase_jd_atom *katom, int js)
@@ -163,7 +164,45 @@ void kbase_job_submit_nolock(kbase_devic
 	 * queue, which I hope will be enough...
 	 */
 	kbasep_jm_enqueue_submit_slot(&jm_slots[js], katom);
+
+#ifdef _TSK_CUSTOM_SCHED_
+
+	if(jm_slots[js].submitted[jm_slots[js].submitted_head & 15] == katom){
+		
+		struct snapshot_kthread_context *sctx;
+		
+		if(katom->sched_stat & SCHED_STAT_PCER){
+			change_snapshot_thread(kbdev);
+			katom->sched_stat &= ~SCHED_STAT_PCER;
+		}
+
+		//trace	
+
+		//앞서 kthread swap될수 있으므로 이후 처리할것.
+		sctx = &kbdev->snapshot_ctx.kthread_ctx[kbdev->snapshot_ctx.skthread_head];
+		if(katom->kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority == SCHED_RT_PRIORITY){
+
+			katom->sched_stat |= SCHED_STAT_RUN;
+			sc_hw_submit(kbdev, katom, js);
+		}else{
+			
+			katom->sched_stat |= SCHED_STAT_SNAP;
+			//sctx->katom = katom;
+
+			sctx->snap_atom_list[sctx->head_pos] = katom;
+			sctx->head_pos = (sctx->head_pos+1)%10;
+			atomic_inc(&sctx->nr_snap_atom);
+			
+		//trace_gpu_custom("kthread - call(submit-b)", ktime_to_ns(ktime_get()), katom->kctx->tInfo.ctx_id, katom->tInfo.atom_id, (u32)katom->sched_stat, (u32)sctx->id);
+			wake_up_interruptible(&sctx->snap_wqueue);
+		//trace_gpu_custom("kthread - call(submit-a)", ktime_to_ns(ktime_get()), katom->kctx->tInfo.ctx_id, katom->tInfo.atom_id, 
+				//(u32)atomic_read(&sctx->nr_snap_atom), (u32)sctx->head_pos);
+			//submit_snapshot((int)kbdev->snapshot_ctx.skthread_head, sctx);
+		}
+	}
+#else
 	kbase_job_hw_submit(kbdev, katom, js);
+#endif
 }
 
 void kbase_job_done_slot(kbase_device *kbdev, int s, u32 completion_code, u64 job_tail, ktime_t *end_timestamp)
@@ -172,6 +211,10 @@ void kbase_job_done_slot(kbase_device *k
 	kbase_jd_atom *katom;
 	mali_addr64 jc_head;
 	kbase_context *kctx;
+#ifdef _TSK_CUSTOM_SCHED_
+	kbase_jd_atom *head_slot_atom;
+	struct snapshot_kthread_context *sctx;
+#endif
 
 	KBASE_DEBUG_ASSERT(kbdev);
 
@@ -186,6 +229,46 @@ void kbase_job_done_slot(kbase_device *k
 	slot = &kbdev->jm_slots[s];
 	katom = kbasep_jm_dequeue_submit_slot(slot);
 
+#ifdef _TSK_CUSTOM_TRACE_
+	//printout_job_trace(katom);
+#endif
+
+#ifdef _TSK_CUSTOM_SCHED_
+if(slot->submitted_nr){
+	head_slot_atom = slot->submitted[slot->submitted_head & 15];
+
+	//run preempt 되어 들어온 job이면 현재 kthread의 sched_stat SNAP->RUN 으로 변경중에 동시 접근 될 수 있다.
+	//하지만 01->10 이 되는 것이므로 사실상 동일한 결과가 된다.
+	if(!(head_slot_atom->sched_stat & (SCHED_STAT_SNAP | SCHED_STAT_RUN))){
+			if(head_slot_atom->kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority == SCHED_RT_PRIORITY){
+
+				head_slot_atom->sched_stat |= SCHED_STAT_RUN;
+				sc_hw_submit(kbdev, head_slot_atom, 1);
+				//kbase_job_hw_submit(kbdev, head_slot_atom, 1);
+
+			}else{
+
+				change_snapshot_thread(kbdev);
+				sctx = &kbdev->snapshot_ctx.kthread_ctx[kbdev->snapshot_ctx.skthread_head];
+				
+				head_slot_atom->sched_stat |= SCHED_STAT_SNAP;
+				//sctx->katom = head_slot_atom;
+			
+				sctx->snap_atom_list[sctx->head_pos] = head_slot_atom;
+				sctx->head_pos = (sctx->head_pos+1)%10;
+				atomic_inc(&sctx->nr_snap_atom);
+				
+		//trace_gpu_custom("kthread - call(done-b)", ktime_to_ns(ktime_get()), head_slot_atom->kctx->tInfo.ctx_id, head_slot_atom->tInfo.atom_id, (u32)head_slot_atom->sched_stat, (u32)sctx->id);
+				wake_up_interruptible(&sctx->snap_wqueue);
+				//submit_snapshot((int)kbdev->snapshot_ctx.skthread_head, sctx);
+		//trace_gpu_custom("kthread - call(done-a)", ktime_to_ns(ktime_get()), head_slot_atom->kctx->tInfo.ctx_id, head_slot_atom->tInfo.atom_id, 
+				//(u32)atomic_read(&sctx->nr_snap_atom), (u32)sctx->head_pos);
+			}
+
+	}
+}
+#endif
+
 	/* If the katom completed is because it's a dummy job for HW workarounds, then take no further action */
 	if (kbasep_jm_is_dummy_workaround_job(kbdev, katom)) {
 		KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_JOB_DONE, NULL, NULL, 0, s, completion_code);
@@ -225,18 +308,26 @@ void kbase_job_done_slot(kbase_device *k
 	 * - Schedule out the parent context if necessary, and schedule a new one in.
 	 */
 #ifdef CONFIG_GPU_TRACEPOINTS
+	/*
 	if (kbasep_jm_nr_jobs_submitted(slot) != 0) {
 		kbase_jd_atom *katom;
 		char js_string[16];
-		katom = kbasep_jm_peek_idx_submit_slot(slot, 0);        /* The atom in the HEAD */
-		//trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(*end_timestamp), (u32)katom->kctx, 0, katom->work_id);
+		katom = kbasep_jm_peek_idx_submit_slot(slot, 0);        // The atom in the HEAD 
+		trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(*end_timestamp), (u32)katom->kctx, 0, katom->work_id);
 		slot->last_context = katom->kctx;
 	} else {
 		char js_string[16];
-		//trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(ktime_get()), 0, 0, 0);
+		trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(ktime_get()), 0, 0, 0);
 		slot->last_context = 0;
-	}
+	}*/
 #endif
+
+#ifdef CONFIG_GPU_TRACEPOINTS
+/*	if(katom->sched_stat & SCHED_STAT_DONE){
+		katom->end = ktime_get();
+		trace_gpu_custom("Job Done", ktime_to_ns(ktime_sub(katom->end, katom->start)), kctx->ctx_id, katom->atom_id, (u32)katom->kctx->nr_dep_job, (u32)katom->sched_stat);
+	}*/
+#endif  
 	kbase_jd_done(katom, s, end_timestamp, KBASE_JS_ATOM_DONE_START_NEW_ATOMS);
 }
 
@@ -281,6 +372,9 @@ void kbase_job_done(kbase_device *kbdev,
 {
 	unsigned long flags;
 	int i;
+#ifdef _TSK_CUSTOM_SCHED_
+	int ret;
+#endif
 	u32 count = 0;
 	ktime_t end_timestamp = ktime_get();
 	kbasep_js_device_data *js_devdata;
@@ -321,10 +415,20 @@ void kbase_job_done(kbase_device *kbdev,
 			u32 active;
 			u32 completion_code = BASE_JD_EVENT_DONE;	/* assume OK */
 			u64 job_tail = 0;
+#ifdef _TSK_CUSTOM_SCHED_
+			ret = sc_is_preempt(slot);
 
+			if (failed & (1u << i) && !ret){// sc_is_preempt(slot)){
+#else
 			if (failed & (1u << i)) {
+#endif
 				/* read out the job slot status code if the job slot reported failure */
 				completion_code = kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_STATUS), NULL);
+				printk(KERN_ALERT"<<job done error>> [%s][%u][%llu] : 0x%08x\n", 
+						slot->submitted[slot->submitted_head & 15]->kctx->tInfo.task->comm,
+						slot->submitted[slot->submitted_head & 15]->kctx->tInfo.ctx_id,
+						slot->submitted[slot->submitted_head & 15]->tInfo.atom_id,
+						completion_code);
 
 				switch (completion_code) {
 				case BASE_JD_EVENT_STOPPED:
@@ -343,6 +447,7 @@ void kbase_job_done(kbase_device *kbdev,
 				}
 			}
 
+			
 			kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), done & ((1 << i) | (1 << (i + 16))), NULL);
 			active = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_JS_STATE), NULL);
 
@@ -379,20 +484,22 @@ void kbase_job_done(kbase_device *kbdev,
 			}
 
 			KBASE_LOG(2, kbdev->dev, "Job ended with status 0x%08X\n", completion_code);
-
+			
+#ifdef _TSK_CUSTOM_SCHED_
+			nr_done = sc_active_jobs(slot);
+#else
 			nr_done = kbasep_jm_nr_jobs_submitted(slot);
 			nr_done -= (active >> i) & 1;
 			nr_done -= (active >> (i + 16)) & 1;
-
+#endif
 			if (nr_done <= 0) {
 				dev_warn(kbdev->dev, "Spurious interrupt on slot %d", i);
 				goto spurious;
 			}
 
 			count += nr_done;
-			kbdev->kernel_end = ktime_get();
-			//printk(KERN_ALERT"[GPU] Kernel Runtime[%d] : %llu : %d - %u\n", count, ktime_to_us(kbdev->kernel_end) - ktime_to_us(kbdev->kernel_start)
-					//, nr_done, active);
+
+
 
 			while (nr_done) {
 				if (nr_done == 1) {
@@ -459,7 +566,11 @@ static mali_bool kbasep_hard_stop_allowe
 	return hard_stops_allowed;
 }
 
+#ifdef _TSK_CUSTOM_SCHED_
+void kbasep_job_slot_soft_or_hard_stop_do_action(kbase_device *kbdev, int js, u32 action, u16 core_reqs, kbase_jd_atom * target_katom )
+#else
 static void kbasep_job_slot_soft_or_hard_stop_do_action(kbase_device *kbdev, int js, u32 action, u16 core_reqs, kbase_jd_atom * target_katom )
+#endif
 {
 	kbase_context *kctx = target_katom->kctx;
 #if KBASE_TRACE_ENABLE
@@ -694,16 +805,24 @@ static void kbasep_job_slot_soft_or_hard
 	
 		if (JM_JOB_IS_CURRENT_JOB_INDEX(jobs_submitted - i)) {
 			/* The last job in the slot, check if there is a job in the next register */
-			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) == 0)
+			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) == 0){
+				//job이 수행중일 때
 				kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, action, core_reqs, katom);
+			}
 			else {
+				//잡이 hw_submit 됬지만, 아직 수행안되고 대기 중일 때
 				/* The job is in the next registers */
 				beenthere(kctx, "clearing job from next registers on slot %d", js);
 				kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
+				//수행 요청 클리어
 				/* Check to see if we did remove a job from the next registers */
-				if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+				if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || 
+						kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+					//잡 수행 요청 클리어했을때 만약 레지스터에 값이 남아있다면, 정상 클리어 한것이다.
 					/* The job was successfully cleared from the next registers, requeue it */
-					kbase_jd_atom *dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+					kbase_jd_atom *dequeued_katom;
+					dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+
 					KBASE_DEBUG_ASSERT(dequeued_katom == katom);
 					jobs_submitted--;
 
@@ -721,6 +840,7 @@ static void kbasep_job_slot_soft_or_hard
 					/* Complete the job, indicate it took no time, but don't submit any more at this point */
 					kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
 				} else {
+					//잡 수행 요청 클리어했을 때 만약 레지스터에 값이 남아있지 않다면 이건 이미 시기를 놓쳐서 수행상태로 들어간것.
 					/* The job transitioned into the current registers before we managed to evict it,
 					 * in this case we fall back to soft/hard-stopping the job */
 					beenthere(kctx, "missed job in next register, soft/hard-stopping slot %d", js);
@@ -735,7 +855,14 @@ static void kbasep_job_slot_soft_or_hard
 			 * has support for this using job chain disambiguation or we need to evict the job
 			 * from the next registers first to ensure we can safely stop the one pointed to by
 			 * the head registers. */
+
+
+#ifdef _TSK_CUSTOM_SCHED_
+			//slot에 2개의 job이 들어 있는 경우 
 			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) != 0) {
+#else
+			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) != 0) {
+#endif
 				kbase_jd_atom *check_next_atom;
 				/* It is - we should remove that job and soft/hard-stop the slot */
 
@@ -764,10 +891,13 @@ static void kbasep_job_slot_soft_or_hard
 					kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
 
 					/* Check to see if we did remove a job from the next registers */
-					if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+					if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || 
+							kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
 						/* We did remove a job from the next registers, requeue it */
-						kbase_jd_atom *dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+						kbase_jd_atom *dequeued_katom;
+					        dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
 						KBASE_DEBUG_ASSERT(dequeued_katom != NULL);
+						
 						jobs_submitted--;
 
 						/* Set the next registers to NULL */
@@ -784,7 +914,6 @@ static void kbasep_job_slot_soft_or_hard
 						continue;
 					}
 				}
-
 				/* Next is now free, so we can soft/hard-stop the slot */
 				beenthere(kctx, "soft/hard-stopped slot %d (there was a job in next which was successfully cleared)\n", js);
 				kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, action, core_reqs, katom);
@@ -795,6 +924,7 @@ static void kbasep_job_slot_soft_or_hard
 		}
 	}
 
+
 	KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, 0);
 }
 
@@ -816,9 +946,12 @@ void kbase_job_kill_jobs_from_context(kb
 	/* Invalidate all jobs in context, to prevent re-submitting */
 	for (i = 0; i < BASE_JD_ATOM_COUNT; i++)
 		kctx->jctx.atoms[i].event_code = BASE_JD_EVENT_JOB_CANCELLED;
+	
+
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++){
 
-	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
 		kbase_job_slot_hardstop(kctx, i, NULL);
+	}
 
 	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
 }
@@ -837,6 +970,7 @@ void kbase_job_zap_context(kbase_context
 	js_devdata = &kbdev->js_data;
 	js_kctx_info = &kctx->jctx.sched_info;
 
+
 	/*
 	 * Critical assumption: No more submission is possible outside of the
 	 * workqueue. This is because the OS *must* prevent U/K calls (IOCTLs)
@@ -934,8 +1068,9 @@ void kbase_job_zap_context(kbase_context
 		KBASE_LOG(2, kbdev->dev, "Zap: Ctx %p Kill Any Running jobs", kctx);
 		/* Cancel any remaining running jobs for this kctx - if any. Submit is disallowed
 		 * which takes effect immediately, so no more new jobs will appear after we do this.  */
-		for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
+		for (i = 0; i < kbdev->gpu_props.num_job_slots; i++){
 			kbase_job_slot_hardstop(kctx, i, NULL);
+		}
 
 		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
 		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
@@ -959,8 +1094,17 @@ mali_error kbase_job_slot_init(kbase_dev
 	int i;
 	KBASE_DEBUG_ASSERT(kbdev);
 
+#ifdef _TSK_CUSTOM_SCHED_
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++){
+		kbdev->jm_slots[i].kbdev = kbdev;
+		kbasep_jm_init_submit_slot(&kbdev->jm_slots[i]);
+	}
+	kbdev->preempt_slot.kbdev = kbdev;
+	kbasep_jm_init_submit_slot(&kbdev->preempt_slot);
+#else
 	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
 		kbasep_jm_init_submit_slot(&kbdev->jm_slots[i]);
+#endif
 
 	return MALI_ERROR_NONE;
 }
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jm.h transcl/drivers/gpu/arm/midgard/mali_kbase_jm.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_jm.h	2018-02-21 16:00:25.639619391 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_jm.h	2018-02-21 15:57:49.498649807 +0900
@@ -29,6 +29,11 @@
 #include <mali_kbase_debug.h>
 #include <linux/atomic.h>
 
+#ifdef _TSK_CUSTOM_SCHED_
+void kbase_job_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js);
+void kbasep_job_slot_soft_or_hard_stop_do_action(kbase_device *kbdev, int js, u32 action, u16 core_reqs, kbase_jd_atom * target_katom );
+#endif
+
 /**
  * @addtogroup base_api
  * @{
@@ -61,6 +66,7 @@ static INLINE int kbasep_jm_is_js_free(k
  */
 static INLINE mali_bool kbasep_jm_is_submit_slots_free(kbase_device *kbdev, int js, kbase_context *kctx)
 {
+
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(0 <= js && js < kbdev->gpu_props.num_job_slots);
 
@@ -68,7 +74,7 @@ static INLINE mali_bool kbasep_jm_is_sub
 		/* The GPU is being reset - so prevent submission */
 		return MALI_FALSE;
 	}
-
+    
 	return (mali_bool) (kbasep_jm_is_js_free(kbdev, js, kctx)
 			    && kbdev->jm_slots[js].submitted_nr < (BASE_JM_SUBMIT_SLOTS - 2));
 }
@@ -101,10 +107,23 @@ static INLINE kbase_jd_atom *kbasep_jm_p
 /**
  * Pop front of the submitted
  */
+#ifdef _TSK_CUSTOM_SCHED_
+static INLINE kbase_jd_atom *kbasep_jm_dequeue_submit_slot(kbase_jm_slot *pslot)
+#else
 static INLINE kbase_jd_atom *kbasep_jm_dequeue_submit_slot(kbase_jm_slot *slot)
+#endif
 {
 	u8 pos;
 	kbase_jd_atom *katom;
+#ifdef _TSK_CUSTOM_SCHED_
+    kbase_device *kbdev = pslot->kbdev;
+    kbase_jm_slot *slot;
+    if(kbdev->preempt_slot.submitted_nr)
+        slot = &kbdev->preempt_slot;
+    else
+        slot = pslot;
+
+#endif
 
 	pos = slot->submitted_head & BASE_JM_SUBMIT_SLOTS_MASK;
 	katom = slot->submitted[pos];
@@ -114,6 +133,9 @@ static INLINE kbase_jd_atom *kbasep_jm_d
 	/* rotate the buffers */
 	slot->submitted_head = (slot->submitted_head + 1) & BASE_JM_SUBMIT_SLOTS_MASK;
 	slot->submitted_nr--;
+#ifdef _TSK_CUSTOM_SCHED_
+    katom->sched_stat &= ~SCHED_STAT_SLOT;
+#endif
 
 	KBASE_LOG(2, katom->kctx->kbdev->dev, "katom %p new head %u", (void *)katom, (unsigned int)slot->submitted_head);
 
@@ -130,6 +152,10 @@ static INLINE kbase_jd_atom *kbasep_jm_d
 
 	pos = (slot->submitted_head + slot->submitted_nr) & BASE_JM_SUBMIT_SLOTS_MASK;
 
+#ifdef _TSK_CUSTOM_SCHED_
+    slot->submitted[pos]->sched_stat &= ~SCHED_STAT_SLOT;
+#endif
+
 	return slot->submitted[pos];
 }
 
@@ -150,6 +176,9 @@ static INLINE void kbasep_jm_enqueue_sub
 
 	pos = (slot->submitted_head + nr) & BASE_JM_SUBMIT_SLOTS_MASK;
 	slot->submitted[pos] = katom;
+#ifdef _TSK_CUSTOM_SCHED_
+    katom->sched_stat |= SCHED_STAT_SLOT;
+#endif
 }
 
 /**
@@ -187,6 +216,7 @@ static INLINE mali_bool kbasep_jm_is_dum
  */
 void kbase_job_submit_nolock(kbase_device *kbdev, kbase_jd_atom *katom, int js);
 
+
 /**
  * @brief Complete the head job on a particular job-slot
  */
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js.c transcl/drivers/gpu/arm/midgard/mali_kbase_js.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js.c	2018-02-21 16:00:25.639619391 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_js.c	2018-02-21 15:57:49.498649807 +0900
@@ -390,6 +390,7 @@ void kbasep_js_try_run_next_job_nolock(k
 	if (js_devdata->nr_user_contexts_running == 0)
 		return; /* No contexts present - the GPU might be powered off, so just return */
 
+
 	for (js = 0; js < kbdev->gpu_props.num_job_slots; ++js)
 		kbasep_js_try_run_next_job_on_slot_nolock(kbdev, js);
 }
@@ -853,6 +854,9 @@ mali_bool kbasep_js_add_job(kbase_contex
 	kbase_device *kbdev;
 	kbasep_js_device_data *js_devdata;
 	kbasep_js_policy *js_policy;
+#ifdef _TSK_CUSTOM_SCHED_
+	u32 skip_by_preemption;
+#endif
 
 	mali_bool policy_queue_updated = MALI_FALSE;
 
@@ -878,6 +882,17 @@ mali_bool kbasep_js_add_job(kbase_contex
 	mutex_lock(&js_devdata->runpool_mutex);
 	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_ADD_JOB, kctx, atom, atom->jc, kbasep_js_trace_get_refcnt(kbdev, kctx));
 
+#ifdef _TSK_CUSTOM_SCHED_
+	skip_by_preemption = atom->sched_stat & SCHED_STAT_PMASK;
+#ifdef CONFIG_GPU_TRACEPOINTS
+	/*if(skip_by_preemption)
+		trace_gpu_custom("preempted job js add", ktime_to_ns(ktime_get()), kctx->ctx_id, atom->atom_id, (u32)atom->sched_stat, (u32)list_empty(&atom->dep_head[0]));
+	else
+		trace_gpu_custom("job js add", ktime_to_ns(ktime_get()), kctx->ctx_id, atom->atom_id, (u32)atom->sched_stat, (u32)list_empty(&atom->dep_head[0]));
+	*/	
+#endif
+#endif
+
 	/* Refcount ctx.nr_jobs */
 	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs < U32_MAX);
 	++(js_kctx_info->ctx.nr_jobs);
@@ -895,6 +910,15 @@ mali_bool kbasep_js_add_job(kbase_contex
 	 * parent context gets scheduled */
 	kbasep_js_policy_enqueue_job(js_policy, atom);
 
+#ifdef _TSK_CUSTOM_SCHED_
+	if(skip_by_preemption){
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+		mutex_unlock(&js_devdata->runpool_mutex);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+		return 1;
+	}
+#endif
+
 	if (js_kctx_info->ctx.is_scheduled != MALI_FALSE) {
 		/* Handle an already running context - try to run the new job, in case it
 		 * matches requirements that aren't matched by any other job in the Run
@@ -1314,6 +1338,7 @@ void kbasep_js_runpool_release_ctx_and_k
 	mutex_unlock(&js_devdata->runpool_mutex);
 	if ((release_result & KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED) != 0u)
 		kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx, MALI_TRUE);
+	
 
 	/* Drop the jsctx_mutex to allow scheduling in a new context */
 	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
@@ -1604,11 +1629,16 @@ mali_bool kbasep_js_try_run_next_job_on_
 		/* Keep submitting while there's space to run a job on this job-slot,
 		 * and there are jobs to get that match its requirements (see 'break'
 		 * statement below) */
+#ifdef _TSK_CUSTOM_SCHED_
+		while (*submit_count < KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ) {
+#else
 		while (*submit_count < KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ && kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE) {
+#endif
 			kbase_jd_atom *dequeued_atom;
 			mali_bool has_job = MALI_FALSE;
 
 			/* Dequeue a job that matches the requirements */
+
 			has_job = kbasep_js_policy_dequeue_job(kbdev, js, &dequeued_atom);
 
 			if (has_job != MALI_FALSE) {
@@ -1625,6 +1655,7 @@ mali_bool kbasep_js_try_run_next_job_on_
 				if (cores_ready != MALI_TRUE && dequeued_atom->event_code != BASE_JD_EVENT_PM_EVENT) {
 					/* The job can't be submitted until the cores are ready, requeue the job */
 					kbasep_js_policy_enqueue_job(&kbdev->js_data.policy, dequeued_atom);
+
 					break;
 				}
 
@@ -1698,7 +1729,9 @@ void kbasep_js_try_run_next_job_on_slot_
 	/* Keep submitting while there's space to run a job on this job-slot,
 	 * and there are jobs to get that match its requirements (see 'break'
 	 * statement below) */
+#ifndef _TSK_CUSTOM_SCHED_
 	if (kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE) {
+#endif
 		/* The caller of this function may not be aware of Ctx Attribute state changes so we
 		 * must recheck if the given slot is still valid. Otherwise do not try to run.
 		 */
@@ -1707,6 +1740,7 @@ void kbasep_js_try_run_next_job_on_slot_
 				kbase_jd_atom *dequeued_atom;
 
 				/* Dequeue a job that matches the requirements */
+
 				has_job = kbasep_js_policy_dequeue_job(kbdev, js, &dequeued_atom);
 
 				if (has_job != MALI_FALSE) {
@@ -1717,6 +1751,7 @@ void kbasep_js_try_run_next_job_on_slot_
 					kbase_context *parent_ctx = dequeued_atom->kctx;
 					mali_bool retain_success;
 
+
 					/* Retain/power up the cores it needs, check if cores are ready */
 					cores_ready = kbasep_js_job_check_ref_cores(kbdev, js, dequeued_atom);
 
@@ -1749,10 +1784,15 @@ void kbasep_js_try_run_next_job_on_slot_
 						kbase_job_submit_nolock(kbdev, dequeued_atom, js);
 					}
 				}
-
+#ifdef _TSK_CUSTOM_SCHED_
+			} while (has_job != MALI_FALSE);
+#else
 			} while (kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE && has_job != MALI_FALSE);
+#endif
 		}
+#ifndef _TSK_CUSTOM_SCHED_
 	}
+#endif
 }
 
 void kbasep_js_try_schedule_head_ctx(kbase_device *kbdev)
@@ -1899,6 +1939,7 @@ void kbasep_js_try_schedule_head_ctx(kba
 	/* Try to run the next job, in case this context has jobs that match the
 	 * job slot requirements, but none of the other currently running contexts
 	 * do */
+
 	kbasep_js_try_run_next_job_nolock(kbdev);
 
 	/* Transaction complete */
@@ -2017,6 +2058,7 @@ void kbasep_js_job_done_slot_irq(kbase_j
 
 	lockdep_assert_held(&js_devdata->runpool_irq.lock);
 
+
 	/*
 	 * Release resources before submitting new jobs (bounds the refcount of
 	 * the resource to BASE_JM_SUBMIT_SLOTS)
@@ -2050,6 +2092,7 @@ void kbasep_js_job_done_slot_irq(kbase_j
 	/* Determine whether the parent context's timeslice is up */
 	if (kbasep_js_policy_should_remove_ctx(js_policy, parent_ctx) != MALI_FALSE)
 		kbasep_js_clear_submit_allowed(js_devdata, parent_ctx);
+	
 
 	if (done_code & KBASE_JS_ATOM_DONE_START_NEW_ATOMS) {
 		/* Submit a new job (if there is one) to help keep the GPU's HEAD and NEXT registers full */
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c transcl/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c	2018-02-21 15:57:49.499649800 +0900
@@ -588,6 +588,10 @@ STATIC mali_bool dequeue_job(kbase_devic
 	kbasep_js_device_data *js_devdata;
 	kbasep_js_policy_cfs *policy_info;
 	kbasep_js_policy_cfs_ctx *ctx_info;
+#ifdef _TSK_CUSTOM_SCHED_
+	int base_priority = kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority;
+	struct kbase_context *tmp_ctx;
+#endif
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(katom_ptr != NULL);
@@ -613,13 +617,52 @@ STATIC mali_bool dequeue_job(kbase_devic
 					KBASE_TRACE_ADD_SLOT(kbdev, JS_POLICY_DEQUEUE_JOB, front_atom->kctx, front_atom, front_atom->jc, job_slot_idx);
 				}
 				*katom_ptr = list_entry(job_list->next, kbase_jd_atom, sched_info.cfs.list);
+
+#ifdef _TSK_CUSTOM_SCHED_
+				if(!sc_preempt(*katom_ptr))
+					return MALI_FALSE;
+
+				(*katom_ptr)->sched_stat &= ~SCHED_STAT_POOL;
+#endif
+
 				list_del(job_list->next);
 
 				(*katom_ptr)->sched_info.cfs.ticks = 0;
 
 				/* Put this context at the back of the Run Pool */
+#ifdef _TSK_CUSTOM_SCHED_
+				/*
+				 * runpool에 있는 ctx 중 선택되어 job이 dequeue되었다면
+				 * runpool 내에 같은 우선순위 그룹의 맨 뒤로 이동한다.
+				 * 이것을 통해 같은 우선순위를 가진 ctx에 대해서는 round-robin 스케줄링을 한다.
+				 */
+				if(kctx->jctx.sched_info.runpool.policy_ctx.cfs.list.next != &policy_info->scheduled_ctxs_head){
+				
+				list_for_each_entry(tmp_ctx, &kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, jctx.sched_info.runpool.policy_ctx.cfs.list){
+
+					if(base_priority < tmp_ctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority){
+						list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+						list_add(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, 
+								tmp_ctx->jctx.sched_info.runpool.policy_ctx.cfs.list.prev);
+
+						break;
+					}
+
+					if(tmp_ctx->jctx.sched_info.runpool.policy_ctx.cfs.list.next == &policy_info->scheduled_ctxs_head){
+						list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+						list_add(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list,
+									&tmp_ctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+						break;
+					}
+
+
+				}
+
+				}
+#else
 				list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
 				list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+#endif
 
 				return MALI_TRUE;
 			}
@@ -644,7 +687,9 @@ STATIC INLINE mali_bool timer_callback_s
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	js_devdata = &kbdev->js_data;
-
+#ifdef _TSK_CUSTOM_SCHED_
+	return 0;
+#endif
 	/* nr_user_contexts_running is updated with the runpool_mutex. However, the
 	 * locking in the caller gives us a barrier that ensures nr_user_contexts is
 	 * up-to-date for reading */
@@ -729,7 +774,6 @@ static enum hrtimer_restart timer_callba
 
 				/* Job is Soft-Stoppable */
 				if (ticks == soft_stop_ticks) {
-					printk(KERN_ALERT"soft_stop_ticks\n");
 					/* Job has been scheduled for at least js_devdata->soft_stop_ticks ticks.
 					 * Soft stop the slot so we can run other jobs.
 					 */
@@ -738,21 +782,26 @@ static enum hrtimer_restart timer_callba
 #if KBASE_DISABLE_SCHEDULING_SOFT_STOPS == 0
 					kbase_job_slot_softstop(kbdev, s, atom);
 #endif
+#ifdef _TSK_CUSTOM_SCHED_
+				} else if (0) {
+#else
 				} else if (ticks == hard_stop_ticks) {
-					printk(KERN_ALERT"hard_stop_ticks\n");
+#endif
 					/* Job has been scheduled for at least js_devdata->hard_stop_ticks_ss ticks.
 					 * It should have been soft-stopped by now. Hard stop the slot.
 					 */
 #if KBASE_DISABLE_SCHEDULING_HARD_STOPS == 0
 					dev_warn(kbdev->dev, "JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)", (unsigned long)ticks, (unsigned long)(js_devdata->scheduling_tick_ns / 1000000u));
+					printk(KERN_ALERT"time out - hardstop 1\n");
 					kbase_job_slot_hardstop(atom->kctx, s, atom);
 #endif
 				} else if (ticks == gpu_reset_ticks) {
-					printk(KERN_ALERT"gpu_reset_ticks\n");
 					/* Job has been scheduled for at least js_devdata->gpu_reset_ticks_ss ticks.
 					 * It should have left the GPU by now. Signal that the GPU needs to be reset.
 					 */
+#ifndef _TSK_CUSTOM_SCHED_
 					reset_needed = MALI_TRUE;
+#endif
 				}
 #else 				/* !CINSTR_DUMPING_ENABLED */
 				/* NOTE: During CINSTR_DUMPING_ENABLED, we use the alternate timeouts, which
@@ -764,16 +813,15 @@ static enum hrtimer_restart timer_callba
 					 */
 					KBASE_LOG(1, kbdev->dev, "Soft-stop");
 				} else if (ticks == js_devdata->hard_stop_ticks_nss) {
-					printk(KERN_ALERT"hard_stop_2_ticks\n");
 					/* Job has been scheduled for at least js_devdata->hard_stop_ticks_nss ticks.
 					 * Hard stop the slot.
 					 */
 #if KBASE_DISABLE_SCHEDULING_HARD_STOPS == 0
 					dev_warn(kbdev->dev, "JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)", (unsigned long)ticks, (unsigned long)(js_devdata->scheduling_tick_ns / 1000000u));
+					printk(KERN_ALERT"time out - hardstop 2\n");
 					kbase_job_slot_hardstop(atom->kctx, s, atom);
 #endif
 				} else if (ticks == js_devdata->gpu_reset_ticks_nss) {
-					printk(KERN_ALERT"gpu_reset_2_ticks\n");
 					/* Job has been scheduled for at least js_devdata->gpu_reset_ticks_nss ticks.
 					 * It should have left the GPU by now. Signal that the GPU needs to be reset.
 					 */
@@ -883,9 +931,11 @@ mali_error kbasep_js_policy_init_ctx(kba
 	if (policy == SCHED_FIFO || policy == SCHED_RR) {
 		ctx_info->process_rt_policy = MALI_TRUE;
 		ctx_info->process_priority = (((MAX_RT_PRIO - 1) - current->rt_priority) / 5) - 20;
+		ctx_info->gpgpu_priority = -1*ctx_info->process_priority-40;
 	} else {
 		ctx_info->process_rt_policy = MALI_FALSE;
 		ctx_info->process_priority = (current->static_prio - MAX_RT_PRIO) - 20;
+		ctx_info->gpgpu_priority = ctx_info->process_priority;
 	}
 
 	ctx_info->bag_total_priority = 0;
@@ -1166,10 +1216,17 @@ void kbasep_js_policy_runpool_add_ctx(kb
 	kbasep_js_policy_cfs *policy_info;
 	kbasep_js_device_data *js_devdata;
 	kbase_device *kbdev;
+#ifdef _TSK_CUSTOM_SCHED_
+	mali_bool scheduled;
+	kbase_context *pos;
+	kbasep_js_policy_cfs_ctx *ctx_info, *runpool_ctx_info;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+#endif
 
 	KBASE_DEBUG_ASSERT(js_policy != NULL);
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 
+
 	policy_info = &js_policy->cfs;
 	js_devdata = container_of(js_policy, kbasep_js_device_data, policy);
 
@@ -1183,7 +1240,30 @@ void kbasep_js_policy_runpool_add_ctx(kb
 	kbasep_js_debug_check(policy_info, kctx, KBASEP_JS_CHECK_NOTSCHEDULED);
 
 	/* All enqueued contexts go to the back of the runpool */
+#ifdef _TSK_CUSTOM_SCHED_
+	if(list_empty(&policy_info->scheduled_ctxs_head)){
+		list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+	}else{
+
+		scheduled = MALI_FALSE;
+		list_for_each_entry(pos, &policy_info->scheduled_ctxs_head, jctx.sched_info.runpool.policy_ctx.cfs.list){
+			runpool_ctx_info = &pos->jctx.sched_info.runpool.policy_ctx.cfs;
+
+			if(runpool_ctx_info->gpgpu_priority > ctx_info->gpgpu_priority){
+			//if(runpool_ctx_info->process_priority > ctx_info->process_priority){
+				list_add(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list,
+						pos->jctx.sched_info.runpool.policy_ctx.cfs.list.prev);
+				scheduled = MALI_TRUE;
+				break;
+			}
+		}
+		if(!scheduled){
+			list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+		}
+	}
+#else
 	list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+#endif
 
 	if (timer_callback_should_run(kbdev) != MALI_FALSE && policy_info->timer_running == MALI_FALSE) {
 		hrtimer_start(&policy_info->scheduling_timer, HR_TIMER_DELAY_NSEC(js_devdata->scheduling_tick_ns), HRTIMER_MODE_REL);
@@ -1352,6 +1432,12 @@ mali_bool kbasep_js_policy_dequeue_job(k
 	kbase_context *kctx;
 	u32 variants_supported;
 	struct list_head *pos;
+#ifdef _TSK_CUSTOM_SCHED_
+	int nr_cur_dep_job = 0;
+	int nr_prev_dep_job = 0;
+	kbase_context *prev_kctx = NULL;
+
+#endif
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(katom_ptr != NULL);
@@ -1360,6 +1446,7 @@ mali_bool kbasep_js_policy_dequeue_job(k
 	js_devdata = &kbdev->js_data;
 	policy_info = &js_devdata->policy.cfs;
 
+
 	/* Get the variants for this slot */
 	if (kbdev->gpu_props.num_core_groups > 1 && kbasep_js_ctx_attr_is_attr_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES) != MALI_FALSE) {
 		/* SS-allcore state, and there's more than one coregroup */
@@ -1383,11 +1470,30 @@ mali_bool kbasep_js_policy_dequeue_job(k
 	/* Second pass through the runpool we consider the non-realtime priority jobs */
 	list_for_each(pos, &policy_info->scheduled_ctxs_head) {
 		kctx = list_entry(pos, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+		
 		if (kctx->jctx.sched_info.runpool.policy_ctx.cfs.process_rt_policy == MALI_FALSE) {
+#ifdef _TSK_CUSTOM_SCHED_
+			if(prev_kctx != NULL)
+				if(kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority >
+						prev_kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority){
+					nr_prev_dep_job = nr_cur_dep_job;
+				}
+
+			if(!atomic_read(&kctx->process_preempt) && !nr_prev_dep_job){
+
+				if (dequeue_job(kbdev, kctx, variants_supported, katom_ptr, job_slot_idx)){ 
+					/* Non-realtime policy job matched */
+					return MALI_TRUE;
+				}
+			}
+			nr_cur_dep_job += kctx->nr_dep_job;
+			prev_kctx = kctx;
+#else
 			if (dequeue_job(kbdev, kctx, variants_supported, katom_ptr, job_slot_idx)) {
-				/* Non-realtime policy job matched */
-				return MALI_TRUE;
+				        /* Non-realtime policy job matched */
+				        return MALI_TRUE;
 			}
+#endif
 		}
 	}
 
@@ -1400,6 +1506,13 @@ void kbasep_js_policy_enqueue_job(kbasep
 	kbasep_js_policy_cfs_job *job_info;
 	kbasep_js_policy_cfs_ctx *ctx_info;
 	kbase_context *parent_ctx;
+#ifdef _TSK_CUSTOM_SCHED_
+	kbase_jd_atom *pos;
+
+	void *klc_map;                  
+	u32* klc_info;
+	struct kbase_va_region* tmp_reg;
+#endif
 
 	KBASE_DEBUG_ASSERT(js_policy != NULL);
 	KBASE_DEBUG_ASSERT(katom != NULL);
@@ -1413,7 +1526,47 @@ void kbasep_js_policy_enqueue_job(kbasep
 		kbase_device *kbdev = container_of(js_policy, kbase_device, js_data.policy);
 		KBASE_TRACE_ADD(kbdev, JS_POLICY_ENQUEUE_JOB, katom->kctx, katom, katom->jc, 0);
 	}
+#ifdef _TSK_CUSTOM_SCHED_
+	if(katom->sched_stat & (SCHED_STAT_PSLOT | SCHED_STAT_PSNAP)){
+		atomic_dec(&parent_ctx->process_preempt); 
+	}else if(katom->sched_stat & SCHED_STAT_PRUN){
+		atomic_dec(&parent_ctx->process_preempt);
+		
+		tmp_reg = kbase_region_tracker_find_region_enclosing_address(parent_ctx, katom->jc_indirect);
+		klc_map = kmap_atomic(pfn_to_page(PFN_DOWN(tmp_reg->alloc->pages[0])));
+		klc_info = klc_map;
+
+		*(klc_info+(katom->jc_indirect&0xfff)/4) = 0x0;
+		kbase_sync_to_memory(tmp_reg->alloc->pages[0], klc_map, PAGE_SIZE);
+		kunmap_atomic(klc_map);
+	}
+
+	if(list_empty(&ctx_info->job_list_head[job_info->cached_variant_idx])){
+		list_add(&katom->sched_info.cfs.list, &ctx_info->job_list_head[job_info->cached_variant_idx]);
+	}else{
+		list_for_each_entry(pos,  &ctx_info->job_list_head[job_info->cached_variant_idx], sched_info.cfs.list){
+			if(pos->tInfo.atom_id>katom->tInfo.atom_id){
+				list_add(&katom->sched_info.cfs.list, pos->sched_info.cfs.list.prev);
+				return;
+			}
+		}
+		list_add_tail(&katom->sched_info.cfs.list, &ctx_info->job_list_head[job_info->cached_variant_idx]);
+	}
+	//|=를 하지 않는 이유는 running stat와 preemption stat를 같이 초기화 하기 위해서다.
+	//RESCHED 상태 활성화를 통해서 재 스케줄링 될때 resnapshot을 찍도록 한다.
+	if(katom->sched_stat & SCHED_STAT_PMASK)
+		katom->sched_stat &= ~(SCHED_STAT_PMASK | SCHED_STAT_SMASK);
+	
+	if(katom->dep_flag){
+		katom->dep_flag = 0;
+		parent_ctx->nr_dep_job--;
+	}
+	
+	katom->sched_stat |= SCHED_STAT_POOL;
+
+#else
 	list_add_tail(&katom->sched_info.cfs.list, &ctx_info->job_list_head[job_info->cached_variant_idx]);
+#endif
 }
 
 void kbasep_js_policy_log_job_result(kbasep_js_policy *js_policy, kbase_jd_atom *katom, u64 time_spent_us)
@@ -1428,7 +1581,6 @@ void kbasep_js_policy_log_job_result(kba
 	KBASE_DEBUG_ASSERT(parent_ctx != NULL);
 
 	ctx_info = &parent_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
-
 	ctx_info->runtime_us += priority_weight(ctx_info, time_spent_us);
 }
 
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h transcl/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h	2018-02-21 15:57:49.499649800 +0900
@@ -135,6 +135,9 @@ typedef struct kbasep_js_policy_cfs_ctx
 	mali_bool process_rt_policy;
 	/* Calling process NICE priority */
 	int process_priority;
+//#ifdef _TSK_CUSTOM_SCHED_
+    int gpgpu_priority;
+//#endif
 	/* Average NICE priority of all atoms in bag:
 	 * Hold the kbasep_js_kctx_info::ctx::jsctx_mutex when accessing  */
 	int bag_priority;
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem.c transcl/drivers/gpu/arm/midgard/mali_kbase_mem.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem.c	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_mem.c	2018-02-21 15:57:49.499649800 +0900
@@ -625,12 +625,45 @@ KBASE_EXPORT_TEST_API(kbase_alloc_free_r
  */
 void kbase_free_alloced_region(struct kbase_va_region *reg)
 {
+#ifdef _TSK_CUSTOM_SNAP_
+	int i=0, s;
+	struct custom_dump_region *dreg;
+#endif
 	KBASE_DEBUG_ASSERT(NULL != reg);
+#ifdef _TSK_CUSTOM_SNAP_
 
-	//measure
-	//reg->kctx->mem_count -= (int)reg->nr_pages;
-	//printk(KERN_ALERT"FREE[%zu] => MEM[%d]\n", reg->nr_pages, reg->kctx->mem_count);
+	//printk(KERN_ALERT"<gpu> del reg : %u\n", reg->reg_id);
+	if(reg->kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority > SCHED_RT_PRIORITY){
+	//spin_lock_irqsave(&reg->kctx->snap_lock, mem_flag);
+	s = kbase_reg_current_backed_size(reg);
+	if (((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_SAME_VA && s==1) ||
+		((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_EXEC && s==1) ||
+		(reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_CUSTOM_VA){
+
+		dreg = &reg->dreg;
+		
+		vunmap(dreg->ksp);
+
+		if(!dreg->is_vmalloc){
+			vunmap(dreg->kdp);
+		
+			for(i=0;i<dreg->nr_pages;i++){
+				__free_page(dreg->dp[i]);
+			}
+		}else{
+			vfree(dreg->kdp);
+		}
+		kfree(dreg->sp);
+		kfree(dreg->dp);
+	}
+
+	//spin_unlock_irqrestore(&reg->kctx->snap_lock, mem_flag);
+	}
+#endif
 	if (!(reg->flags & KBASE_REG_FREE)) {
+#ifdef _TSK_CUSTOM_TRACE_
+		context_trace_release_reg(reg->kctx, reg->alloc->nents);
+#endif
 		kbase_mem_phy_alloc_put(reg->alloc);
 		KBASE_DEBUG_CODE(
 					/* To detect use-after-free in debug builds */
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem.h transcl/drivers/gpu/arm/midgard/mali_kbase_mem.h
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem.h	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_mem.h	2018-02-21 15:57:49.499649800 +0900
@@ -177,6 +177,13 @@ static inline struct kbase_mem_phy_alloc
  * A GPU memory region, and attributes for CPU mappings.
  */
 typedef struct kbase_va_region {
+#ifdef _TSK_CUSTOM_TRACE_
+    reg_trace tInfo;
+#endif
+#ifdef _TSK_CUSTOM_SNAP_
+    custom_dump_region dreg;
+#endif
+
 	struct rb_node rblink;
 	struct list_head link;
 
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c transcl/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c	2018-02-21 15:57:49.500649793 +0900
@@ -40,6 +40,7 @@
 #include <mali_kbase_mem_linux.h>
 #include <mali_kbase_config_defaults.h>
 
+
 static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_struct *vma);
 static const struct vm_operations_struct kbase_vm_ops;
 
@@ -50,6 +51,11 @@ struct kbase_va_region *kbase_mem_alloc(
 	int cpu_va_bits;
 	struct kbase_va_region *reg;
 	struct device *dev;
+#ifdef _TSK_CUSTOM_SNAP_
+	int i, s;
+	struct custom_dump_region *dreg;
+	//unsigned long mem_flag;
+#endif
 	KBASE_DEBUG_ASSERT(kctx);
 	KBASE_DEBUG_ASSERT(flags);
 	KBASE_DEBUG_ASSERT(gpu_va);
@@ -153,11 +159,53 @@ struct kbase_va_region *kbase_mem_alloc(
 		*gpu_va = reg->start_pfn << PAGE_SHIFT;
 	}
 
-	kbase_gpu_vm_unlock(kctx);
-	if((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_CUSTOM_VA){
-		kctx->mem_count += (int)va_pages;
-		//printk(KERN_ALERT"ALLOC[%llu] => MEM[%d]\n", va_pages, reg->kctx->mem_count);
+#ifdef _TSK_CUSTOM_SNAP_
+
+	if(kctx->jctx.sched_info.runpool.policy_ctx.cfs.gpgpu_priority > SCHED_RT_PRIORITY){
+		//spin_lock_irqsave(&kctx->snap_lock, mem_flag);
+
+#ifdef _TSK_CUSTOM_TRACE_
+		context_trace_init_reg(kctx, reg);
+#endif
+		s =  kbase_reg_current_backed_size(reg);
+		if (((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_SAME_VA && s==1) ||
+			((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_EXEC && s==1) ||
+			(reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_CUSTOM_VA){
+
+			dreg = &reg->dreg;
+			dreg->kctx = reg->alloc->imported.kctx;
+			dreg->reg = reg;
+			dreg->nr_pages = reg->alloc->nents;
+			dreg->sp = kmalloc(sizeof(struct page*) * dreg->nr_pages, GFP_KERNEL);
+			dreg->dp = kmalloc(sizeof(struct page*) * dreg->nr_pages, GFP_KERNEL);
+
+			for(i=0;i<dreg->nr_pages;i++){
+				dreg->sp[i] = pfn_to_page(PFN_DOWN(reg->alloc->pages[i]));
+				dreg->dp[i] = alloc_page(GFP_HIGHUSER);
+			/*mp = kmap(dreg->dp[i]);
+			memset(mp, 0x00, 1);
+			kbase_sync_to_memory(PFN_PHYS(page_to_pfn(dreg->dp[i])), mp, 1);
+			kunmap(dreg->dp[i]);
+			*/
+			}
+			
+			dreg->ksp = vmap(dreg->sp, dreg->nr_pages, VM_MAP, PAGE_KERNEL);
+			if(dreg->ksp==NULL)
+				printk(KERN_ALERT"ksp null : %zu\n",dreg->nr_pages);
+			dreg->kdp = vmap(dreg->dp, dreg->nr_pages, VM_MAP, PAGE_KERNEL);
+			if(dreg->kdp==NULL)
+				printk(KERN_ALERT"kdp null : %zu\n",dreg->nr_pages);
+	                
+
+			//printk(KERN_ALERT"<gpu> region alloc : %u\n",reg->reg_id);
+		}
+#ifdef _TSK_CUSTOM_TRACE_
+		context_trace_alloc_done_reg(kctx, reg);
+#endif
+		//spin_unlock_irqrestore(&kctx->snap_lock, mem_flag);
 	}
+#endif
+	kbase_gpu_vm_unlock(kctx);
 	return reg;
 
 no_mmap:
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mmu.c transcl/drivers/gpu/arm/midgard/mali_kbase_mmu.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_mmu.c	2018-02-21 16:00:25.640619385 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_mmu.c	2018-02-21 15:57:49.500649793 +0900
@@ -1317,6 +1317,9 @@ void kbase_mmu_interrupt(kbase_device *k
 		KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&as->work_pagefault));
 		INIT_WORK(&as->work_pagefault, page_fault_worker);
 		queue_work(as->pf_wq, &as->work_pagefault);
+#ifdef CONFIG_GPU_TRACEPOINTS
+		//trace_gpu_custom("page fault", ktime_to_ns(ktime_get()), kctx->ctx_id, 0, (u32)pf_bits, (u32)0);
+#endif
 	}
 
 	/* reenable interrupts */
diff -urpN odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_trace_timeline.c transcl/drivers/gpu/arm/midgard/mali_kbase_trace_timeline.c
--- odxu3_3.10.72_original/drivers/gpu/arm/midgard/mali_kbase_trace_timeline.c	2018-02-21 16:00:25.641619378 +0900
+++ transcl/drivers/gpu/arm/midgard/mali_kbase_trace_timeline.c	2018-02-21 15:57:49.501649787 +0900
@@ -134,6 +134,7 @@ void kbase_timeline_job_slot_done(kbase_
 {
 	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
 
+	printk(KERN_ALERT"time line?\n");
 	if (done_code & KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT) {
 		KBASE_TIMELINE_JOB_START_NEXT(kctx, js, 0);
 	} else {
diff -urpN odxu3_3.10.72_original/err transcl/err
--- odxu3_3.10.72_original/err	2018-02-21 16:00:26.883611182 +0900
+++ transcl/err	1970-01-01 09:00:00.000000000 +0900
@@ -1,70 +0,0 @@
-sound/soc/samsung/lpass.c: In function ‘lpass_proc_show’:
-sound/soc/samsung/lpass.c:718:5: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 3 has type ‘void *’ [-Wformat=]
-     ar->reg, ar->val);
-     ^
-drivers/hardkernel/ioboard-spi.c:215:12: warning: ‘ioboard_spi_read_memory’ defined but not used [-Wunused-function]
- static int ioboard_spi_read_memory              (struct spi_device *spi, unsigned int addr, unsigned char *rdata, unsigned int size)
-            ^
-drivers/hardkernel/ioboard-spi.c:332:13: warning: ‘ioboard_spi_test’ defined but not used [-Wunused-function]
- static void ioboard_spi_test        (struct spi_device *spi)
-             ^
-drivers/hid/hid-appleir.c:347:2: warning: initialization from incompatible pointer type [enabled by default]
-  .input_configured = appleir_input_configured,
-  ^
-drivers/hid/hid-appleir.c:347:2: warning: (near initialization for ‘appleir_driver.input_configured’) [enabled by default]
-net/netfilter/nfnetlink_queue_core.c: In function ‘nfqnl_zcopy’:
-net/netfilter/nfnetlink_queue_core.c:264:2: warning: passing argument 1 of ‘skb_orphan_frags’ discards ‘const’ qualifier from pointer target type [enabled by default]
-  if (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {
-  ^
-In file included from net/netfilter/nfnetlink_queue_core.c:18:0:
-include/linux/skbuff.h:1922:19: note: expected ‘struct sk_buff *’ but argument is of type ‘const struct sk_buff *’
- static inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)
-                   ^
-net/netfilter/nfnetlink_queue_core.c:265:3: warning: passing argument 1 of ‘skb_tx_error’ discards ‘const’ qualifier from pointer target type [enabled by default]
-   skb_tx_error(from);
-   ^
-In file included from net/netfilter/nfnetlink_queue_core.c:18:0:
-include/linux/skbuff.h:622:13: note: expected ‘struct sk_buff *’ but argument is of type ‘const struct sk_buff *’
- extern void skb_tx_error(struct sk_buff *skb);
-             ^
-drivers/gpu/drm/exynos/exynos_drm_drv.c: In function ‘exynos_drm_init’:
-drivers/gpu/drm/exynos/exynos_drm_drv.c:768:1: warning: label ‘err_unregister_pd’ defined but not used [-Wunused-label]
- err_unregister_pd:
- ^
-drivers/hid/hid-magicmouse.c:590:2: warning: initialization from incompatible pointer type [enabled by default]
-  .input_configured = magicmouse_input_configured,
-  ^
-drivers/hid/hid-magicmouse.c:590:2: warning: (near initialization for ‘magicmouse_driver.input_configured’) [enabled by default]
-drivers/hid/hid-ntrig.c:1026:2: warning: initialization from incompatible pointer type [enabled by default]
-  .input_configured = ntrig_input_configured,
-  ^
-drivers/hid/hid-ntrig.c:1026:2: warning: (near initialization for ‘ntrig_driver.input_configured’) [enabled by default]
-drivers/gpu/drm/exynos/exynos_hdmi.c:1705:13: warning: ‘hdmiphy_poweroff’ defined but not used [-Wunused-function]
- static void hdmiphy_poweroff(struct hdmi_context *hdata)
-             ^
-net/rfkill/rfkill-regulator.c: In function ‘rfkill_regulator_set_block’:
-net/rfkill/rfkill-regulator.c:43:20: warning: ignoring return value of ‘regulator_enable’, declared with attribute warn_unused_result [-Wunused-result]
-    regulator_enable(rfkill_data->vcc);
-                    ^
-drivers/media/platform/exynos/mfc/s5p_mfc.c:2108:28: warning: ‘s5p_mfc_dec_drm_videodev’ defined but not used [-Wunused-variable]
- static struct video_device s5p_mfc_dec_drm_videodev = {
-                            ^
-drivers/media/platform/exynos/mfc/s5p_mfc.c:2115:28: warning: ‘s5p_mfc_enc_drm_videodev’ defined but not used [-Wunused-variable]
- static struct video_device s5p_mfc_enc_drm_videodev = {
-                            ^
-drivers/usb/host/xhci.c: In function ‘xhci_free_dev’:
-drivers/usb/host/xhci.c:3506:17: warning: unused variable ‘dev’ [-Wunused-variable]
-  struct device *dev = hcd->self.controller;
-                 ^
-drivers/usb/host/xhci.c: In function ‘xhci_alloc_dev’:
-drivers/usb/host/xhci.c:3592:17: warning: unused variable ‘dev’ [-Wunused-variable]
-  struct device *dev = hcd->self.controller;
-                 ^
-drivers/usb/host/xhci-plat.c: In function ‘xhci_plat_probe’:
-drivers/usb/host/xhci-plat.c:156:4: warning: ISO C90 forbids mixed declarations and code [-Wdeclaration-after-statement]
-    extern void samsung_usb3phy_retune(int);
-    ^
-arch/arm/boot/compressed/atags_to_fdt.c: In function ‘merge_fdt_bootargs’:
-arch/arm/boot/compressed/atags_to_fdt.c:96:1: warning: the frame size of 1032 bytes is larger than 1024 bytes [-Wframe-larger-than=]
- }
- ^
diff -urpN odxu3_3.10.72_original/.git/config transcl/.git/config
--- odxu3_3.10.72_original/.git/config	2018-02-21 16:00:24.828624743 +0900
+++ transcl/.git/config	2018-02-21 15:57:48.581655858 +0900
@@ -4,7 +4,7 @@
 	bare = false
 	logallrefupdates = true
 [remote "origin"]
-	url = ssh://hyunsu@115.145.179.240:5001/volume1/homes/hyunsu/git/original/odxu3_3.10.72_original
+	url = https://devops.skku.edu/gitlab/GPU/transcl.git
 	fetch = +refs/heads/*:refs/remotes/origin/*
 [branch "master"]
 	remote = origin
Binary files odxu3_3.10.72_original/.git/index and transcl/.git/index differ
diff -urpN odxu3_3.10.72_original/.git/logs/HEAD transcl/.git/logs/HEAD
--- odxu3_3.10.72_original/.git/logs/HEAD	2018-02-21 16:00:24.827624750 +0900
+++ transcl/.git/logs/HEAD	2018-02-21 15:57:48.581655858 +0900
@@ -1 +1 @@
-0000000000000000000000000000000000000000 d4af490d029af447a20dabb560c0078e8fc5d575 Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196424 +0900	clone: from ssh://hyunsu@115.145.179.240:5001/volume1/homes/hyunsu/git/original/odxu3_3.10.72_original
+0000000000000000000000000000000000000000 c4ffa186d265d5b7d719c52cc0b37aa63f7d6ecc Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196268 +0900	clone: from https://devops.skku.edu/gitlab/GPU/transcl.git
diff -urpN odxu3_3.10.72_original/.git/logs/refs/heads/master transcl/.git/logs/refs/heads/master
--- odxu3_3.10.72_original/.git/logs/refs/heads/master	2018-02-21 16:00:24.827624750 +0900
+++ transcl/.git/logs/refs/heads/master	2018-02-21 15:57:48.581655858 +0900
@@ -1 +1 @@
-0000000000000000000000000000000000000000 d4af490d029af447a20dabb560c0078e8fc5d575 Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196424 +0900	clone: from ssh://hyunsu@115.145.179.240:5001/volume1/homes/hyunsu/git/original/odxu3_3.10.72_original
+0000000000000000000000000000000000000000 c4ffa186d265d5b7d719c52cc0b37aa63f7d6ecc Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196268 +0900	clone: from https://devops.skku.edu/gitlab/GPU/transcl.git
diff -urpN odxu3_3.10.72_original/.git/logs/refs/remotes/origin/HEAD transcl/.git/logs/refs/remotes/origin/HEAD
--- odxu3_3.10.72_original/.git/logs/refs/remotes/origin/HEAD	2018-02-21 16:00:24.826624756 +0900
+++ transcl/.git/logs/refs/remotes/origin/HEAD	2018-02-21 15:57:48.581655858 +0900
@@ -1 +1 @@
-0000000000000000000000000000000000000000 d4af490d029af447a20dabb560c0078e8fc5d575 Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196424 +0900	clone: from ssh://hyunsu@115.145.179.240:5001/volume1/homes/hyunsu/git/original/odxu3_3.10.72_original
+0000000000000000000000000000000000000000 c4ffa186d265d5b7d719c52cc0b37aa63f7d6ecc Hyeonsu-Lee <hyeonsu.lee@csl.skku.edu> 1519196268 +0900	clone: from https://devops.skku.edu/gitlab/GPU/transcl.git
Binary files odxu3_3.10.72_original/.git/objects/pack/pack-2e85a329305db4206cbf0c3f59dc692855ad3eac.idx and transcl/.git/objects/pack/pack-2e85a329305db4206cbf0c3f59dc692855ad3eac.idx differ
Binary files odxu3_3.10.72_original/.git/objects/pack/pack-2e85a329305db4206cbf0c3f59dc692855ad3eac.pack and transcl/.git/objects/pack/pack-2e85a329305db4206cbf0c3f59dc692855ad3eac.pack differ
Binary files odxu3_3.10.72_original/.git/objects/pack/pack-750f7069ca7ec72012770bd5c9fb5fe3dfed2527.idx and transcl/.git/objects/pack/pack-750f7069ca7ec72012770bd5c9fb5fe3dfed2527.idx differ
Binary files odxu3_3.10.72_original/.git/objects/pack/pack-750f7069ca7ec72012770bd5c9fb5fe3dfed2527.pack and transcl/.git/objects/pack/pack-750f7069ca7ec72012770bd5c9fb5fe3dfed2527.pack differ
diff -urpN odxu3_3.10.72_original/.git/packed-refs transcl/.git/packed-refs
--- odxu3_3.10.72_original/.git/packed-refs	2018-02-21 16:00:24.826624756 +0900
+++ transcl/.git/packed-refs	2018-02-21 15:57:48.580655865 +0900
@@ -1,2 +1,2 @@
 # pack-refs with: peeled fully-peeled 
-d4af490d029af447a20dabb560c0078e8fc5d575 refs/remotes/origin/master
+c4ffa186d265d5b7d719c52cc0b37aa63f7d6ecc refs/remotes/origin/master
diff -urpN odxu3_3.10.72_original/.git/refs/heads/master transcl/.git/refs/heads/master
--- odxu3_3.10.72_original/.git/refs/heads/master	2018-02-21 16:00:24.827624750 +0900
+++ transcl/.git/refs/heads/master	2018-02-21 15:57:48.581655858 +0900
@@ -1 +1 @@
-d4af490d029af447a20dabb560c0078e8fc5d575
+c4ffa186d265d5b7d719c52cc0b37aa63f7d6ecc
Binary files odxu3_3.10.72_original/include/linux/.hrtimer.h.swp and transcl/include/linux/.hrtimer.h.swp differ
diff -urpN odxu3_3.10.72_original/include/linux/syscalls.h transcl/include/linux/syscalls.h
--- odxu3_3.10.72_original/include/linux/syscalls.h	2018-02-21 16:00:27.099609756 +0900
+++ transcl/include/linux/syscalls.h	2018-02-21 15:57:50.909640495 +0900
@@ -846,5 +846,4 @@ asmlinkage long sys_process_vm_writev(pi
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
-asmlinkage long sys_mem_alloc_test(size_t mem_size);
 #endif
diff -urpN odxu3_3.10.72_original/include/trace/events/gpu.h transcl/include/trace/events/gpu.h
--- odxu3_3.10.72_original/include/trace/events/gpu.h	2018-02-21 16:00:27.128609565 +0900
+++ transcl/include/trace/events/gpu.h	2018-02-21 15:57:50.941640284 +0900
@@ -55,7 +55,7 @@
  * working on.  This should correspond to a job_id that was previously traced
  * as a gpu_job_enqueue event when the batch of work was created.
  */
-TRACE_EVENT(gpu_custom,                                                                               
+TRACE_EVENT(gpu_custom,
         TP_PROTO(const char *func_name, u64 timestamp, u32 ctx_id, u64 atom_id, u32 data1, u32 data2),
 
         TP_ARGS(func_name, timestamp, ctx_id, atom_id, data1, data2),
@@ -68,7 +68,6 @@ TRACE_EVENT(gpu_custom,
             __field(        u32,            data1           )
             __field(        u32,            data2           )
         ),
-        
         TP_fast_assign(
             __assign_str(func_name, func_name);
             __entry->timestamp = timestamp;
@@ -77,8 +76,7 @@ TRACE_EVENT(gpu_custom,
             __entry->data1 = data1;
             __entry->data2 = data2;
         ),
-        
-        TP_printk("%llu.%06lu - %s : ctx[ %u ] atom[ %llu ] data1[ 0x%08x ] data2[ 0x%08x ]",
+        TP_printk("[%llu.%06lu] - [%s] : ctx[ %u ] atom[ %llu ] data1[ %u ] data2[ %u ]",
             (unsigned long long)show_secs_from_ns(__entry->timestamp),
             (unsigned long)show_usecs_from_ns(__entry->timestamp),
             __get_str(func_name),
@@ -86,8 +84,58 @@ TRACE_EVENT(gpu_custom,
             __entry->atom_id,
             __entry->data1,
             __entry->data2)
-);                                                                                                    
+);
+
+TRACE_EVENT(gpu_custom_bench,
+        TP_PROTO(const char*func_name, u32 ctx_id, u64 using_time, u64 snapshot_time, u64 matimes, u64 ktimes, u32 snapshot_pages, u32 mapages, u32 nr_kernel, u32 preemption1, u32 preemption2),
+        
+        TP_ARGS(func_name, ctx_id, using_time, snapshot_time, matimes, ktimes, snapshot_pages, mapages, nr_kernel, preemption1, preemption2),
+        
+        TP_STRUCT__entry(
+            __string(   func_name, func_name    )
+            __field(    u32,    ctx_id  )
+            __field(    u64,    using_time  )
+            __field(    u64,    snapshot_time   )
+            __field(    u64,    matimes   )
+            __field(    u64,    ktimes   )
+            __field(    u32,    snapshot_pages  )
+            __field(    u32,    mapages  )
+            __field(    u32,    nr_kernel  )
+            __field(    u32,    preemption1 )
+            __field(    u32,    preemption2 )
+        ),
 
+        TP_fast_assign(
+            __assign_str(func_name,func_name);
+            __entry->ctx_id = ctx_id;
+            __entry->using_time = using_time;
+            __entry->snapshot_time = snapshot_time;
+            __entry->matimes = matimes;
+            __entry->ktimes = ktimes;
+            __entry->snapshot_pages = snapshot_pages;
+            __entry->mapages = mapages;
+            __entry->nr_kernel = nr_kernel;
+            __entry->preemption1 = preemption1;
+            __entry->preemption2 = preemption2;
+        ),
+
+        TP_printk("[ %s ]-(%03u) : [ %llu.%06lu ] [ %llu.%06lu ] [ %llu.%06lu ] [ %llu.%06lu ] [ %u ] [ %u ] [ %u ] [ %u ] [ %u ]",
+            __get_str(func_name),
+            __entry->ctx_id,
+            (unsigned long long)show_secs_from_ns(__entry->using_time),
+            (unsigned long)show_usecs_from_ns(__entry->using_time),
+            (unsigned long long)show_secs_from_ns(__entry->snapshot_time),
+            (unsigned long)show_usecs_from_ns(__entry->snapshot_time),
+            (unsigned long long)show_secs_from_ns(__entry->matimes),
+            (unsigned long)show_usecs_from_ns(__entry->matimes),
+            (unsigned long long)show_secs_from_ns(__entry->ktimes),
+            (unsigned long)show_usecs_from_ns(__entry->ktimes),
+            __entry->snapshot_pages,
+            __entry->mapages,
+            __entry->nr_kernel, 
+            __entry->preemption1,
+            __entry->preemption2)
+);
 
 TRACE_EVENT(gpu_sched_switch,
 
diff -urpN odxu3_3.10.72_original/kernel/Makefile transcl/kernel/Makefile
--- odxu3_3.10.72_original/kernel/Makefile	2018-02-21 16:00:27.183609202 +0900
+++ transcl/kernel/Makefile	2018-02-21 15:57:50.978640040 +0900
@@ -10,8 +10,7 @@ obj-y     = fork.o exec_domain.o panic.o
 	    kthread.o wait.o sys_ni.o posix-cpu-timers.o mutex.o \
 	    hrtimer.o rwsem.o nsproxy.o srcu.o semaphore.o \
 	    notifier.o ksysfs.o cred.o \
-	    async.o range.o groups.o lglock.o smpboot.o \
-	    mem_alloc_test.o
+	    async.o range.o groups.o lglock.o smpboot.o
 
 ifdef CONFIG_FUNCTION_TRACER
 # Do not trace debug files and internal ftrace files
Binary files odxu3_3.10.72_original/kernel/.user.c.swp and transcl/kernel/.user.c.swp differ
diff -urpN odxu3_3.10.72_original/mm/vmalloc.c transcl/mm/vmalloc.c
--- odxu3_3.10.72_original/mm/vmalloc.c	2018-02-21 16:00:27.222608945 +0900
+++ transcl/mm/vmalloc.c	2018-02-21 15:57:51.022639750 +0900
@@ -1584,7 +1584,7 @@ void *vmap(struct page **pages, unsigned
 	area = get_vm_area_caller((count << PAGE_SHIFT), flags,
 					__builtin_return_address(0));
 	if (!area){
-		printk(KERN_ALERT"area null!!\n");
+		printk(KERN_ALERT"<gpu> !area\n");
 		return NULL;
 	}
 
